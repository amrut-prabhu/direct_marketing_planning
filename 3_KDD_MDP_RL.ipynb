{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = True # whether it is the training data\n",
    "data_dir = './rl_data/train/' if is_train else './rl_data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (2385300, 40)\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    mdp_data = pd.read_csv('./jmp/train/all_data.csv')\n",
    "    print('Train data:', mdp_data.shape)\n",
    "else:\n",
    "    mdp_data = pd.read_csv('./data/test/mdp_raw_data-imputed.csv')\n",
    "    print('Test data:', mdp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_train:\n",
    "#     jmp_data = pd.read_csv('./jmp/train/imputed_mdp_state_data-full_3_6_22.csv')\n",
    "#     print('Train data:', jmp_data.shape)\n",
    "# else:\n",
    "#     jmp_data = pd.read_csv('./jmp/test/imputed_mdp_state_data-full_3_6_22.csv')\n",
    "#     print('Test data:', jmp_data.shape)\n",
    "\n",
    "# display(jmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'action', 'reward', 'bloc', 'date', 'age', 'income_bracket',\n",
       "       'Type_Cluster_3', 'Type_Cluster_5', 'Type_Cluster_10',\n",
       "       'num_gifts_to_date', 'num_promotions_to_date',\n",
       "       'frequency-gifts_per_prom', 'last_gift_amount',\n",
       "       'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
       "       'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
       "       'recent_amount_per_prom', 'months_since_last_gift',\n",
       "       'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
       "       'gift_recency_ratio', 'prom_recency_ratio',\n",
       "       'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
       "       'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
       "       'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago',\n",
       "       'Cluster 1 Components', 'Cluster 2 Components', 'Cluster 3 Components',\n",
       "       'Cluster 4 Components', 'Cluster 5 Components', 'State_Cluster_4',\n",
       "       'State_Cluster_6', 'State_Cluster_9', 'State_Cluster_11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 6\n",
    "\n",
    "mdp_data['state'] = mdp_data['State_Cluster_' + str(num_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_columns 29 \n",
      "\n",
      "type_features 2\n",
      "state_features 21\n",
      "removed 6\n"
     ]
    }
   ],
   "source": [
    "all_columns = [\n",
    "    'num_gifts_to_date', 'num_promotions_to_date',\n",
    "    'frequency-gifts_per_prom', 'last_gift_amount',\n",
    "    'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
    "    'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
    "    'recent_amount_per_prom', 'months_since_last_gift',\n",
    "    'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
    "    'gift_recency_ratio', 'prom_recency_ratio',\n",
    "    'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
    "    'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
    "    'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago', \n",
    "    \n",
    "    'age', 'income_bracket', \n",
    "\n",
    "#     'type', \n",
    "    'state',\n",
    "\n",
    "    'id', 'action', \n",
    "    'reward', 'bloc', 'date'\n",
    "]\n",
    "\n",
    "type_features = [\n",
    "    'age', 'income_bracket',\n",
    "]\n",
    "\n",
    "state_features = [\n",
    "    'num_gifts_to_date', 'num_promotions_to_date',\n",
    "    'frequency-gifts_per_prom', 'last_gift_amount',\n",
    "    'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
    "    'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
    "    'recent_amount_per_prom', 'months_since_last_gift',\n",
    "    'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
    "    'gift_recency_ratio', 'prom_recency_ratio',\n",
    "    'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
    "    'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
    "    'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago', \n",
    "]\n",
    "\n",
    "removed = [\n",
    "    'id', 'state', # 'type', \n",
    "    'action', 'date', 'bloc', 'reward', \n",
    "]\n",
    "\n",
    "print('all_columns', len(all_columns), '\\n')\n",
    "print('type_features', len(type_features))\n",
    "print('state_features', len(state_features))\n",
    "print('removed', len(removed))\n",
    "\n",
    "assert len(all_columns) == len(type_features) + len(state_features) + len(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE_COST = -0.68 # Cost (negative reward) for taking each action\n",
    "\n",
    "# Change actions to numbers\n",
    "ACTION_0 = 'no_action'\n",
    "ACTION_1 = 'labels_only'\n",
    "ACTION_2 = 'thank_you_with_labels'\n",
    "ACTION_3 = 'calendars_with_stickers'\n",
    "ACTION_4 = 'blank_cards_with_labels'\n",
    "ACTION_5 = 'greeting_cards_with_labels'\n",
    "ACTION_6 = 'labels_and_notepad'\n",
    "\n",
    "ACTIONS = {\n",
    "    ACTION_0: 0,\n",
    "    ACTION_1: 1,\n",
    "    ACTION_2: 2,\n",
    "    ACTION_3: 3,\n",
    "    ACTION_4: 4,\n",
    "    ACTION_5: 5,\n",
    "    ACTION_6: 6,\n",
    "}\n",
    "\n",
    "ACTION_NAMES = {\n",
    "    0: ACTION_0,\n",
    "    1: ACTION_1,\n",
    "    2: ACTION_2,\n",
    "    3: ACTION_3,\n",
    "    4: ACTION_4,\n",
    "    5: ACTION_5,\n",
    "    6: ACTION_6,\n",
    "}\n",
    "\n",
    "# TODO: FIXME: change to 4 actions instead of 2\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_0, 'action'] = ACTIONS[ACTION_0]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_1, 'action'] = ACTIONS[ACTION_1]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_2, 'action'] = ACTIONS[ACTION_2]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_3, 'action'] = ACTIONS[ACTION_3]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_4, 'action'] = ACTIONS[ACTION_4]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_5, 'action'] = ACTIONS[ACTION_5]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_6, 'action'] = ACTIONS[ACTION_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>Type_Cluster_3</th>\n",
       "      <th>Type_Cluster_5</th>\n",
       "      <th>Type_Cluster_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Cluster 1 Components</th>\n",
       "      <th>Cluster 2 Components</th>\n",
       "      <th>Cluster 3 Components</th>\n",
       "      <th>Cluster 4 Components</th>\n",
       "      <th>Cluster 5 Components</th>\n",
       "      <th>State_Cluster_4</th>\n",
       "      <th>State_Cluster_6</th>\n",
       "      <th>State_Cluster_9</th>\n",
       "      <th>State_Cluster_11</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95515</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>09-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.940385</td>\n",
       "      <td>3.094768</td>\n",
       "      <td>-0.681861</td>\n",
       "      <td>2.081938</td>\n",
       "      <td>0.598063</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>10-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.082934</td>\n",
       "      <td>-0.654150</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>12-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.082934</td>\n",
       "      <td>-0.450526</td>\n",
       "      <td>-0.487073</td>\n",
       "      <td>0.547664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>02-1995</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.072428</td>\n",
       "      <td>-0.598728</td>\n",
       "      <td>-0.400118</td>\n",
       "      <td>0.522465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95515</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>04-1995</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.063180</td>\n",
       "      <td>-0.571017</td>\n",
       "      <td>0.767771</td>\n",
       "      <td>0.497266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385295</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>12-1997</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.427811</td>\n",
       "      <td>-0.430789</td>\n",
       "      <td>6.379600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385296</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>02-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.631435</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.354401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385297</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>04-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.835059</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.329202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385298</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>06-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>1.038683</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.304003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385299</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>08-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>1.242307</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>6.278803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2385300 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc     date  age  income_bracket  \\\n",
       "0         95515      5    0.00     0  09-1994   60               3   \n",
       "1         95515      6   -0.68     1  10-1994   60               3   \n",
       "2         95515      0    0.00     2  12-1994   60               3   \n",
       "3         95515      6   -0.68     3  02-1995   60               3   \n",
       "4         95515      2   -0.68     4  04-1995   60               3   \n",
       "...         ...    ...     ...   ...      ...  ...             ...   \n",
       "2385295  185114      0    0.00    20  12-1997   80               5   \n",
       "2385296  185114      0    0.00    21  02-1998   80               5   \n",
       "2385297  185114      0    0.00    22  04-1998   80               5   \n",
       "2385298  185114      0    0.00    23  06-1998   80               5   \n",
       "2385299  185114      0    0.00    24  08-1998   80               5   \n",
       "\n",
       "         Type_Cluster_3  Type_Cluster_5  Type_Cluster_10  ...  \\\n",
       "0                     1               1                1  ...   \n",
       "1                     1               1                1  ...   \n",
       "2                     1               1                1  ...   \n",
       "3                     1               1                1  ...   \n",
       "4                     1               1                1  ...   \n",
       "...                 ...             ...              ...  ...   \n",
       "2385295               2               4                8  ...   \n",
       "2385296               2               4                8  ...   \n",
       "2385297               2               4                8  ...   \n",
       "2385298               2               4                8  ...   \n",
       "2385299               2               4                8  ...   \n",
       "\n",
       "         Cluster 1 Components  Cluster 2 Components  Cluster 3 Components  \\\n",
       "0                    3.940385              3.094768             -0.681861   \n",
       "1                   -0.926485              3.082934             -0.654150   \n",
       "2                   -0.926485              3.082934             -0.450526   \n",
       "3                   -0.926485              3.072428             -0.598728   \n",
       "4                   -0.926485              3.063180             -0.571017   \n",
       "...                       ...                   ...                   ...   \n",
       "2385295             -0.926485             18.638946              0.427811   \n",
       "2385296             -0.926485             18.638946              0.631435   \n",
       "2385297             -0.926485             18.638946              0.835059   \n",
       "2385298             -0.926485             18.638946              1.038683   \n",
       "2385299             -0.926485             18.638946              1.242307   \n",
       "\n",
       "         Cluster 4 Components  Cluster 5 Components  State_Cluster_4  \\\n",
       "0                    2.081938              0.598063                3   \n",
       "1                   -1.685633              0.572864                0   \n",
       "2                   -0.487073              0.547664                0   \n",
       "3                   -0.400118              0.522465                0   \n",
       "4                    0.767771              0.497266                0   \n",
       "...                       ...                   ...              ...   \n",
       "2385295             -0.430789              6.379600                0   \n",
       "2385296             -1.368910              6.354401                0   \n",
       "2385297             -1.368910              6.329202                0   \n",
       "2385298             -1.368910              6.304003                0   \n",
       "2385299             -1.685633              6.278803                0   \n",
       "\n",
       "         State_Cluster_6  State_Cluster_9  State_Cluster_11  state  \n",
       "0                      5                5                 7      5  \n",
       "1                      0                0                 1      0  \n",
       "2                      0                0                 1      0  \n",
       "3                      0                0                 1      0  \n",
       "4                      0                0                 1      0  \n",
       "...                  ...              ...               ...    ...  \n",
       "2385295                0                5                 1      0  \n",
       "2385296                0                0                 1      0  \n",
       "2385297                0                0                 1      0  \n",
       "2385298                0                0                 1      0  \n",
       "2385299                0                0                 1      0  \n",
       "\n",
       "[2385300 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mdp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states: 6\n",
      "num_actions: 7\n",
      "========================== Processed 0 rows ( 0.0 %)========================== 0.03434810638427734 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 2.7404181957244873 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 5.751573979854584 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 9.239826651414235 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 11.896366314093273 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 15.09619671901067 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 18.69744060834249 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 22.157708791891732 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 25.569531460603077 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 29.035609900951385 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 32.578945370515186 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 35.979737337430315 mins\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_states = max(mdp_data['state']) + 1 # 0 based index\n",
    "num_actions = max(mdp_data['action']) + 1 # 0 based index\n",
    "\n",
    "print('num_states:', num_states)\n",
    "print('num_actions:', num_actions)\n",
    "\n",
    "# transitions[curr_state][action][next_state]\n",
    "transition_counts = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# transitions[curr_state][action]\n",
    "transition_sums = np.zeros((num_states, num_actions))\n",
    "\n",
    "# rewards[curr_state][action][next_state]\n",
    "rewards = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for i, row in mdp_data.iterrows():\n",
    "    if i + 1 >= len(mdp_data) or mdp_data.iloc[i + 1]['bloc'] == 0:\n",
    "        # Skip since there is no transition to make\n",
    "        continue\n",
    "\n",
    "    curr_s = row['state']\n",
    "    action = row['action']\n",
    "    next_s = mdp_data.iloc[i + 1]['state']\n",
    "    reward = mdp_data.iloc[i + 1]['reward']\n",
    "\n",
    "    transition_counts[curr_s, action, next_s] += 1\n",
    "    rewards[curr_s, action, next_s] += reward\n",
    "\n",
    "    transition_sums[curr_s, action] += 1\n",
    "\n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(mdp_data) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transition_counts, open(data_dir + 'transition_counts_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(transition_sums, open(data_dir + 'transition_sums_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 7\n",
    "\n",
    "# transition_counts = pickle.load(open(data_dir + 'transition_counts_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "# transition_sums = pickle.load(open(data_dir + 'transition_sums_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# Rare transition threshold\n",
    "# TRANSITION_THRESHOLD = 5\n",
    "\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        if transition_sums[s][a] == 0:\n",
    "            transitions[s][a][:] = 0\n",
    "            \n",
    "            assert np.isclose(np.sum(rewards[s][a][:]), 0)\n",
    "        else:\n",
    "            transitions[s][a][:] = transition_counts[s][a][:] / transition_sums[s][a]\n",
    "            rewards[s][a][:] = rewards[s][a][:] / transition_sums[s][a]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transitions, open(data_dir + 'transitions_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(rewards, open(data_dir + 'rewards_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 7\n",
    "\n",
    "# transitions = pickle.load(open(data_dir + 'transitions_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "# rewards = pickle.load(open(data_dir + 'rewards_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning on MDP\n",
    "\n",
    "https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919\n",
    "\n",
    "https://www.datahubbs.com/reinforcement-learning-markov-decision-processes/  \n",
    "We can set this up just like we did for the MDP example above in order to solve for ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
    "\n",
    "\"\"\"\n",
    "Implements the following:\n",
    "- MDP class\n",
    "- plot_graph \n",
    "- plot_graph_with_state_values\n",
    "- plot_graph_optimal_strategy_and_state_values\n",
    "\"\"\"\n",
    "\n",
    "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self, state=None):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if state:\n",
    "            self._current_state = state\n",
    "            return self._current_state\n",
    "        \n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        state_node = str(state_node)\n",
    "        \n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            posible_action = str(posible_action)\n",
    "            \n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    try:\n",
    "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
    "                    action in next_actions]\n",
    "        optimal_action = next_actions[np.argmax(q_values)]\n",
    "    except NameError:\n",
    "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
    "        \n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp,\n",
    "                                                     state_values,\n",
    "                                                     state_node,\n",
    "                                                     gamma):\n",
    "                graph.edge(str(state_node), str(state_node) + \"-\" + str(action),\n",
    "                           **opt_s_a_edge_attrs)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      action (string) : action that is taken \n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      q (float) : Returns action-value: Expected return starting from state s, \n",
    "                  taking action a, and then following policy π\n",
    "    \"\"\"\n",
    "    \n",
    "    q = 0\n",
    "    next_states = mdp.get_next_states(state,action)\n",
    "    \n",
    "    for next_state in next_states:\n",
    "        probability = next_states[next_state]\n",
    "        reward = mdp.get_reward(state,action,next_state)\n",
    "        \n",
    "        q += probability * (reward + gamma * state_values[next_state])\n",
    "    \n",
    "    return q\n",
    "\n",
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      v (float) : Returns next state value: Expected return for next iteration, \n",
    "                  starting from state s, and then following policy π\n",
    "    \"\"\"\n",
    "\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    \n",
    "    v = 0\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if action_value > v:\n",
    "            v = action_value\n",
    "\n",
    "    return v\n",
    "\n",
    "def value_iteration(mdp, gamma, num_iter, min_difference, state_values=None):\n",
    "    \"\"\"\n",
    "    Performs num_iter value iteration steps starting from state_values.\n",
    "\n",
    "    Args:\n",
    "      mdp : the MDP that we're using\n",
    "      gamma (float) : discount factor for MDP\n",
    "      num_iter (string) : maximum number of iterations\n",
    "      min_difference (float) : stop Value Iteration if new values are at least this close to old values\n",
    "      state_values (dict- {string->float}) : state-values for the states\n",
    "\n",
    "    Returns:\n",
    "      state_values (dict) : Returns the state-values at the end of Value Iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize V(s) to 0 if argument is not provided\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "    new_state_values = {}\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        for s in state_values:\n",
    "            new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        state_values = dict(new_state_values)\n",
    "\n",
    "        if i % (0.1 * num_iter) == 0:\n",
    "            print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "            print(\"  \".join(\"V(%s) = %.3f\" % (s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if diff < min_difference:\n",
    "            print(\"Terminated\")\n",
    "            break\n",
    "            \n",
    "    return state_values\n",
    "\n",
    "def get_optimal_action(mdp, state_values, state, gamma=1, debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      optimal_action (string) : Returns the optimal action - action that results \n",
    "                                in the maximum action-value.\n",
    "    \"\"\"\n",
    "\n",
    "    if mdp.is_terminal(state): \n",
    "        return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    # Compute optimal action as per formula above. \n",
    "    optimal_action = None\n",
    "    optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "    if debug:\n",
    "        print()\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "\n",
    "        if action_value >= optimal_action_value:\n",
    "            optimal_action_value = action_value\n",
    "            optimal_action = action\n",
    "            \n",
    "            if debug:\n",
    "                print('action', action, ':', action_value)\n",
    "\n",
    "    return optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_dict = {}\n",
    "rewards_dict = {}\n",
    "\n",
    "for curr_s in range(transitions.shape[0]):\n",
    "    transitions_dict[curr_s] = {}\n",
    "    rewards_dict[curr_s] = {}\n",
    "    for a in range(transitions.shape[1]):        \n",
    "        if math.isclose(sum(transitions[curr_s][a][:]), 0):\n",
    "            continue\n",
    "        \n",
    "        transitions_dict[curr_s][a] = {}\n",
    "        rewards_dict[curr_s][a] = {}\n",
    "        for next_s in range(transitions.shape[2]):\n",
    "            transitions_dict[curr_s][a][next_s] = transitions[curr_s][a][next_s].item()\n",
    "            rewards_dict[curr_s][a][next_s] = rewards[curr_s][a][next_s].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions:\n",
      "{0: {0: {0: 0.9750720611822683,\n",
      "         1: 0.0004823450103442665,\n",
      "         2: 9.879355633557265e-05,\n",
      "         3: 1.452846416699598e-05,\n",
      "         4: 0.0006072898021804319,\n",
      "         5: 0.02372498198470443},\n",
      "     1: {0: 0.29775280898876405,\n",
      "         1: 0.0,\n",
      "         2: 0.028089887640449437,\n",
      "         3: 0.0,\n",
      "         4: 0.40730337078651685,\n",
      "         5: 0.26685393258426965},\n",
      "     2: {0: 0.4441637190500253,\n",
      "         1: 0.003705575206333165,\n",
      "         2: 0.0018527876031665825,\n",
      "         3: 0.0,\n",
      "         4: 0.32693279434057604,\n",
      "         5: 0.22334512379989893},\n",
      "     3: {0: 0.4827586206896552,\n",
      "         1: 0.0,\n",
      "         2: 0.05172413793103448,\n",
      "         3: 0.0,\n",
      "         4: 0.034482758620689655,\n",
      "         5: 0.43103448275862066},\n",
      "     4: {0: 0.8345545102379635,\n",
      "         1: 0.020962921970116215,\n",
      "         2: 0.005467625899280576,\n",
      "         3: 0.0,\n",
      "         4: 0.10085224128389596,\n",
      "         5: 0.03816270060874377},\n",
      "     5: {0: 0.5862706813277155,\n",
      "         1: 0.01048196485458843,\n",
      "         2: 0.011715137190422362,\n",
      "         3: 0.0,\n",
      "         4: 0.1300996814304799,\n",
      "         5: 0.26143253519679377},\n",
      "     6: {0: 0.5859120521172638,\n",
      "         1: 0.010956470239857862,\n",
      "         2: 0.026317737636955878,\n",
      "         3: 0.0,\n",
      "         4: 0.1639769025762511,\n",
      "         5: 0.2128368374296713}},\n",
      " 1: {0: {0: 0.007697360418440579,\n",
      "         1: 0.9576961440214086,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.00047196204841260187,\n",
      "         5: 0.03413453351173823},\n",
      "     1: {0: 0.0012755102040816326,\n",
      "         1: 0.19005102040816327,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.5127551020408163,\n",
      "         5: 0.29591836734693877},\n",
      "     2: {0: 0.0028017241379310344,\n",
      "         1: 0.33189655172413796,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.3762931034482759,\n",
      "         5: 0.28900862068965516},\n",
      "     3: {0: 0.023255813953488372,\n",
      "         1: 0.6744186046511628,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.023255813953488372,\n",
      "         5: 0.27906976744186046},\n",
      "     4: {0: 7.151796888968353e-05,\n",
      "         1: 0.8312533524047917,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.07922403003754694,\n",
      "         5: 0.08945109958877168},\n",
      "     5: {0: 0.003473845948293139,\n",
      "         1: 0.6285657024517336,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.07589017302425012,\n",
      "         5: 0.29207027857572315},\n",
      "     6: {0: 0.009002811509242089,\n",
      "         1: 0.4954836394089849,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.2432254591134773,\n",
      "         5: 0.25228808996829577}},\n",
      " 2: {0: {0: 0.012538151201543588,\n",
      "         1: 0.024308016137519733,\n",
      "         2: 0.9563830906858446,\n",
      "         3: 0.0022487282932818803,\n",
      "         4: 0.004511489212418874,\n",
      "         5: 1.0524469391334854e-05},\n",
      "     1: {0: 0.004529616724738676,\n",
      "         1: 0.010801393728222997,\n",
      "         2: 0.7222996515679443,\n",
      "         3: 0.0,\n",
      "         4: 0.26236933797909406,\n",
      "         5: 0.0},\n",
      "     2: {0: 0.0047622765243188385,\n",
      "         1: 0.011320165508626747,\n",
      "         2: 0.9544070575376689,\n",
      "         3: 0.0,\n",
      "         4: 0.029510500429385588,\n",
      "         5: 0.0},\n",
      "     3: {0: 0.005284791544333529,\n",
      "         1: 0.48209042865531415,\n",
      "         2: 0.5126247798003524,\n",
      "         3: 0.0,\n",
      "         4: 0.0,\n",
      "         5: 0.0},\n",
      "     4: {0: 0.001117259920046087,\n",
      "         1: 0.009985510535411902,\n",
      "         2: 0.9249166419356528,\n",
      "         3: 0.0,\n",
      "         4: 0.0639805876088892,\n",
      "         5: 0.0},\n",
      "     5: {0: 0.04410158383397051,\n",
      "         1: 0.08619606772255597,\n",
      "         2: 0.8525395958492626,\n",
      "         3: 0.0,\n",
      "         4: 0.017135445111960676,\n",
      "         5: 2.7307482250136537e-05},\n",
      "     6: {0: 0.06851360714883042,\n",
      "         1: 0.10317062098296419,\n",
      "         2: 0.7628246481733687,\n",
      "         3: 0.0,\n",
      "         4: 0.06545528397008578,\n",
      "         5: 3.583972475091391e-05}},\n",
      " 3: {0: {0: 0.001734093712569498,\n",
      "         1: 0.03740815560362551,\n",
      "         2: 0.11474515973327136,\n",
      "         3: 0.8293794804869764,\n",
      "         4: 0.0,\n",
      "         5: 0.016733110463557216}},\n",
      " 4: {0: {0: 0.1490168829461586,\n",
      "         1: 0.16116734586565007,\n",
      "         2: 0.018267948693872185,\n",
      "         3: 0.0,\n",
      "         4: 0.4569052939961314,\n",
      "         5: 0.21464252849818777},\n",
      "     1: {0: 0.0008179959100204499,\n",
      "         1: 0.00027266530334015,\n",
      "         2: 0.001226993865030675,\n",
      "         3: 0.0,\n",
      "         4: 0.85453306066803,\n",
      "         5: 0.14314928425357873},\n",
      "     2: {0: 0.00787897888433659,\n",
      "         1: 0.0016388276079420108,\n",
      "         2: 6.303183107469272e-05,\n",
      "         3: 0.0,\n",
      "         4: 0.8925307280176489,\n",
      "         5: 0.0978884336589978},\n",
      "     4: {0: 0.0009753087648411142,\n",
      "         1: 0.0005573192941949225,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.9523691046068411,\n",
      "         5: 0.04609826733412287},\n",
      "     5: {0: 0.20842150723385877,\n",
      "         1: 0.05147916216799827,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.6504858561865687,\n",
      "         5: 0.08961347441157418},\n",
      "     6: {0: 0.0022724173976275963,\n",
      "         1: 0.004203972185611053,\n",
      "         2: 0.0002499659137390356,\n",
      "         3: 0.0,\n",
      "         4: 0.8655865109303277,\n",
      "         5: 0.12768713357269462}},\n",
      " 5: {0: {0: 0.08048243514884657,\n",
      "         1: 0.16943620178041544,\n",
      "         2: 0.0,\n",
      "         3: 9.572125969177754e-06,\n",
      "         4: 0.09028429214128458,\n",
      "         5: 0.6597874988034843},\n",
      "     1: {0: 0.0,\n",
      "         1: 0.00042408821034775233,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.4751908396946565,\n",
      "         5: 0.5243850720949957},\n",
      "     2: {0: 0.001049653158086893,\n",
      "         1: 0.0067999269806498725,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.4308141657539248,\n",
      "         5: 0.5613362541073385},\n",
      "     3: {0: 0.0,\n",
      "         1: 0.0,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.05555555555555555,\n",
      "         5: 0.9444444444444444},\n",
      "     4: {0: 0.0016481069042316259,\n",
      "         1: 0.003637713437268003,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.5436525612472161,\n",
      "         5: 0.45106161841128434},\n",
      "     5: {0: 0.20816393984465378,\n",
      "         1: 0.18096182449181952,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.22151710461080812,\n",
      "         5: 0.3893571310527186},\n",
      "     6: {0: 0.0011264096685705198,\n",
      "         1: 0.0057248114920289955,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.33011754416188493,\n",
      "         5: 0.6630312346775156}}}\n",
      "\n",
      "Rewards:\n",
      "{0: {0: {0: 0.04305248843531498,\n",
      "         1: -9.681768520886119e-05,\n",
      "         2: -2.173458239382598e-05,\n",
      "         3: 0.0,\n",
      "         4: 0.0006736558264952678,\n",
      "         5: 0.01327407657081825},\n",
      "     1: {0: 0.8316853932584253,\n",
      "         1: 0.0,\n",
      "         2: -0.017191011235955053,\n",
      "         3: 0.0,\n",
      "         4: 0.9178651685393249,\n",
      "         5: 0.6837640449438194},\n",
      "     2: {0: 1.23099039919148,\n",
      "         1: -0.0025197911403065515,\n",
      "         2: -0.0010308236483072258,\n",
      "         3: 0.0,\n",
      "         4: 0.5258295435405276,\n",
      "         5: 0.662526528549786},\n",
      "     3: {0: 0.2182758620689656,\n",
      "         1: 0.0,\n",
      "         2: -0.017586206896551725,\n",
      "         3: 0.0,\n",
      "         4: 0.09689655172413794,\n",
      "         5: 1.0693103448275851},\n",
      "     4: {0: 0.020000442722745305,\n",
      "         1: -0.0001505257332595462,\n",
      "         2: -0.0009182069728832316,\n",
      "         3: 0.0,\n",
      "         4: 0.04724560044272251,\n",
      "         5: 0.017261759822911036},\n",
      "     5: {0: 0.6261720275408368,\n",
      "         1: -0.005730140787175006,\n",
      "         2: -0.005380741958688725,\n",
      "         3: 0.0,\n",
      "         4: 0.11243243243243331,\n",
      "         5: 0.13835782550611503},\n",
      "     6: {0: 2.311282943440857,\n",
      "         1: -0.00357417826473202,\n",
      "         2: -0.004002072845721069,\n",
      "         3: 0.0,\n",
      "         4: 0.29897986378442193,\n",
      "         5: 0.5375795824696283}},\n",
      " 1: {0: {0: 0.006786425009123032,\n",
      "         1: 0.011294611361150427,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.0020779953776912815,\n",
      "         5: 0.018068629120543273},\n",
      "     1: {0: 0.012755102040816327,\n",
      "         1: 0.5273469387755093,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 2.6245408163265282,\n",
      "         5: 0.43586734693877505},\n",
      "     2: {0: 0.03968965517241379,\n",
      "         1: 0.4875366379310339,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.5624267241379417,\n",
      "         5: 0.226112068965523},\n",
      "     3: {0: 0.24000000000000002,\n",
      "         1: 0.45209302325581396,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.0,\n",
      "         5: 0.10604651162790699},\n",
      "     4: {0: 0.0005893080636509923,\n",
      "         1: 0.04964115859109673,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.05819417128553748,\n",
      "         5: 0.06776828178079881},\n",
      "     5: {0: 0.014411116307034533,\n",
      "         1: 0.22545193399694052,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.179453537310443,\n",
      "         5: 0.13928251720221357},\n",
      "     6: {0: 0.13164323742298278,\n",
      "         1: 1.1072309624932035,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.6587129867798905,\n",
      "         5: 0.3732706227193639}},\n",
      " 2: {0: {0: 0.13903729170320794,\n",
      "         1: 0.3632725486756995,\n",
      "         2: -0.185924714962358,\n",
      "         3: 0.0,\n",
      "         4: 0.0852893176635669,\n",
      "         5: 0.005610664795649886},\n",
      "     1: {0: 0.11772822299651566,\n",
      "         1: 0.18993728222996512,\n",
      "         2: -0.09737979094076726,\n",
      "         3: 0.0,\n",
      "         4: 4.270452961672472,\n",
      "         5: 0.0},\n",
      "     2: {0: 0.09224139277070817,\n",
      "         1: 0.19094386759309873,\n",
      "         2: -0.6464485908346755,\n",
      "         3: 0.0,\n",
      "         4: 0.5598625966117547,\n",
      "         5: 0.0},\n",
      "     3: {0: 0.05869641808573107,\n",
      "         1: 4.246200822078684,\n",
      "         2: -0.05709923664122147,\n",
      "         3: 0.0,\n",
      "         4: 0.0,\n",
      "         5: 0.0},\n",
      "     4: {0: 0.016659043695337192,\n",
      "         1: 0.16026046121886037,\n",
      "         2: -0.17638950473966342,\n",
      "         3: 0.0,\n",
      "         4: 0.18523959988128927,\n",
      "         5: 0.0},\n",
      "     5: {0: 0.408273484434725,\n",
      "         1: 1.149234025122932,\n",
      "         2: -0.4919694156200597,\n",
      "         3: 0.0,\n",
      "         4: 0.22218145821954924,\n",
      "         5: 0.007500273074822501},\n",
      "     6: {0: 1.064206747425546,\n",
      "         1: 1.554017513678915,\n",
      "         2: -0.24289776121192774,\n",
      "         3: 0.0,\n",
      "         4: 1.1048295223759417,\n",
      "         5: 0.02353475258643347}},\n",
      " 3: {0: {0: 0.012450792856248977,\n",
      "         1: 0.390568049770268,\n",
      "         2: -0.07817866528417285,\n",
      "         3: 0.5603155335466685,\n",
      "         4: 0.0,\n",
      "         5: 0.007518815810644089}},\n",
      " 4: {0: {0: 0.024151533362128986,\n",
      "         1: 0.04474130969595351,\n",
      "         2: -0.0034224116884823763,\n",
      "         3: 0.0,\n",
      "         4: 0.1681355987905537,\n",
      "         5: 0.06628307573848746},\n",
      "     1: {0: 0.0043081117927743695,\n",
      "         1: -9.270620313565099e-05,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 3.9948289025221415,\n",
      "         5: 0.33501295160190875},\n",
      "     2: {0: 0.008804286164513045,\n",
      "         1: -0.0003580208005042547,\n",
      "         2: -4.2861645130791055e-05,\n",
      "         3: 0.0,\n",
      "         4: 0.28107721399309965,\n",
      "         5: 0.1501178695241093},\n",
      "     4: {0: -0.0003101083786984602,\n",
      "         1: 0.00012738726724455376,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.08040584787170854,\n",
      "         5: 0.0384181088962111},\n",
      "     5: {0: 0.11074929820773168,\n",
      "         1: 0.009299071474843303,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.606740660764315,\n",
      "         5: 0.07601641114230243},\n",
      "     6: {0: -0.0002526928146161886,\n",
      "         1: -0.002315138844702996,\n",
      "         2: -0.0002317865745580148,\n",
      "         3: 0.0,\n",
      "         4: 1.078092305594525,\n",
      "         5: 0.013404535745125326}},\n",
      " 5: {0: {0: 0.016158514406049512,\n",
      "         1: 0.01446999138508654,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.05698592897482387,\n",
      "         5: 0.40698104719046274},\n",
      "     1: {0: 0.0,\n",
      "         1: 0.009541984732824428,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 1.5634775233248472,\n",
      "         5: 1.8738761662425751},\n",
      "     2: {0: 0.0032730923694779084,\n",
      "         1: -0.00459291712303762,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.34952217962757026,\n",
      "         5: 0.6438462942678774},\n",
      "     3: {0: 0.0,\n",
      "         1: 0.0,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.5177777777777778,\n",
      "         5: 1.2088888888888891},\n",
      "     4: {0: -0.0006224201930215291,\n",
      "         1: 0.0011272457312546385,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: -0.0344763177431316,\n",
      "         5: 0.3346262806235498},\n",
      "     5: {0: 0.29294397620225926,\n",
      "         1: 0.1153885308213604,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.1670639563708498,\n",
      "         5: 0.3827440092546248},\n",
      "     6: {0: 0.001582274287380236,\n",
      "         1: 0.0076707173241807,\n",
      "         2: 0.0,\n",
      "         3: 0.0,\n",
      "         4: 0.4990171081750278,\n",
      "         5: 0.5413212122816701}}}\n"
     ]
    }
   ],
   "source": [
    "print('Transitions:')\n",
    "pprint.pprint(transitions_dict)\n",
    "\n",
    "print('\\nRewards:')\n",
    "pprint.pprint(rewards_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run KDD Marketing RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "initial_states = np.arange(num_states)\n",
    "mdp = MDP(transitions_dict, rewards_dict, initial_state=random.choice(initial_states), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.46167   |   V(0) = 1.518  V(1) = 1.575  V(2) = 2.018  V(3) = 0.471  V(4) = 3.462  V(5) = 1.726\n",
      "\n",
      "iter  100   |   diff: 0.01807   |   V(0) = 57.025  V(1) = 58.557  V(2) = 56.735  V(3) = 45.947  V(4) = 61.334  V(5) = 58.625\n",
      "\n",
      "iter  200   |   diff: 0.00011   |   V(0) = 57.367  V(1) = 58.898  V(2) = 57.076  V(3) = 46.288  V(4) = 61.675  V(5) = 58.966\n",
      "\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "mdp.reset()\n",
    "\n",
    "# Discount factor γ / gamma\n",
    "# If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward. \n",
    "# If γ=1, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
    "\n",
    "gamma = 0.95\n",
    "num_iter = 1000\n",
    "min_difference = 1e-5\n",
    "\n",
    "state_values = value_iteration(mdp, gamma, num_iter, min_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MDP state values after running Value Iteration:\n",
      "{0: 57.368496523566634,\n",
      " 1: 58.900064913261374,\n",
      " 2: 57.078237569478766,\n",
      " 3: 46.289614983801194,\n",
      " 4: 61.67708966230235,\n",
      " 5: 58.96823574423879}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final MDP state values after running Value Iteration:\")\n",
    "pprint.pprint(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action for each MDP state\n",
      "\n",
      "action 0 : 54.58142974898012\n",
      "action 1 : 57.368505643297205\n",
      "    MDP State 0 : optimal action  1 labels_only\n",
      "\n",
      "action 0 : 55.95880462197499\n",
      "action 1 : 58.90007403299194\n",
      "    MDP State 1 : optimal action  1 labels_only\n",
      "\n",
      "action 0 : 54.09967862497296\n",
      "action 1 : 56.44322034401039\n",
      "action 3 : 57.07824668920934\n",
      "    MDP State 2 : optimal action  3 calendars_with_stickers\n",
      "\n",
      "action 0 : 46.289624103531764\n",
      "    MDP State 3 : optimal action  0 no_action\n",
      "\n",
      "action 0 : 57.02771930305913\n",
      "action 1 : 61.67709878203292\n",
      "    MDP State 4 : optimal action  1 labels_only\n",
      "\n",
      "action 0 : 56.396178993716305\n",
      "action 1 : 58.96824486396936\n",
      "    MDP State 5 : optimal action  1 labels_only\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal action for each MDP state\")\n",
    "\n",
    "for state in range(num_states):\n",
    "    value = state_values[state]\n",
    "    optimal_action_id = get_optimal_action(mdp, state_values, state, gamma, debug=True)\n",
    "    print(\"    MDP State\", state, \": optimal action \", optimal_action_id, ACTION_NAMES[optimal_action_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transitions_dict, open(data_dir + 'mdp_transitions_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(rewards_dict, open(data_dir + 'mdp_rewards_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(state_values, open(data_dir + 'mdp_state_values_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 4\n",
    "\n",
    "# transitions_dict = pickle.load(open('./mdp_data/transitions_dict' + str(num_states) + '.p', \"rb\"))\n",
    "# rewards_dict = pickle.load(open('./mdp_data/rewards_dict' + str(num_states) + '.p', \"rb\"))\n",
    "# state_values = pickle.load(open('./mdp_data/state_values' + str(num_states) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 25 # total num months / TIME_PERIOD of each event\n",
    "\n",
    "def get_average_reward(start_state, mdp, state_values, gamma):\n",
    "    \"\"\"\n",
    "    Returns the average reward for the episode of length EPISODE_LENGTH \n",
    "    starting at `start_state`, when following the OPTIMAL policy. \n",
    "    \"\"\"\n",
    "    s = start_state\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "    \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10000 episodes: 2.9735369058997136\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward over random episodes\n",
    "num_samples = 10000\n",
    "\n",
    "avg_rewards = []\n",
    "for _ in range(num_samples):\n",
    "    start_s = mdp.reset() # select random start state\n",
    "\n",
    "    r = get_average_reward(start_s, mdp, state_values, gamma)\n",
    "    avg_rewards.append(r)\n",
    "\n",
    "print(\"Average reward over\", num_samples, \"episodes:\", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95515</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95515</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385295</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385296</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385297</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385298</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385299</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2385300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc  state\n",
       "0         95515      5    0.00     0      5\n",
       "1         95515      6   -0.68     1      0\n",
       "2         95515      0    0.00     2      0\n",
       "3         95515      6   -0.68     3      0\n",
       "4         95515      2   -0.68     4      0\n",
       "...         ...    ...     ...   ...    ...\n",
       "2385295  185114      0    0.00    20      0\n",
       "2385296  185114      0    0.00    21      0\n",
       "2385297  185114      0    0.00    22      0\n",
       "2385298  185114      0    0.00    23      0\n",
       "2385299  185114      0    0.00    24      0\n",
       "\n",
       "[2385300 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdp_episodes = mdp_data[['id', 'action', 'reward', 'bloc', 'state']]\n",
    "\n",
    "sample_episodes = mdp_episodes\n",
    "display(sample_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for original dataset policy:  0.5396969018572084\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following original data policy\n",
    "\n",
    "print(\"Average reward for original dataset policy: \", np.mean(sample_episodes['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 564.2212461709976 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 565.1266188542048 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 566.111942867438 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 566.9223411202431 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 567.7579773505529 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 568.5636686523756 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 569.5545958638191 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 570.3443955024084 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 571.1061064163844 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 571.7977150082588 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 572.5397132555644 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 573.2706241567929 mins\n",
      "Average reward for optimal RL policy:  2.803267089625766\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following RL optimal policy\n",
    "T = 50 # length of episode\n",
    "\n",
    "avg_rewards = []\n",
    "for i, row in sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    s = row['state']\n",
    "    mdp.reset(s) # Set start state to `s`\n",
    "\n",
    "    r = get_average_reward(s, mdp, state_values, gamma)\n",
    "    avg_rewards.append(r)\n",
    "\n",
    "print(\"Average reward for optimal RL policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 573.8936297933261 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 574.493492881457 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 575.153084119161 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 575.7485195875167 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 576.3584539890289 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 576.9454550504685 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 577.574689924717 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 578.1631288647652 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 578.7657262444496 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 579.356665802002 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 579.949921866258 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 580.9159218033154 mins\n",
      "Average reward for random policy:  0.7036050761026232\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following random policy\n",
    "\n",
    "avg_rewards = []\n",
    "for i, row in sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    \n",
    "    s = row['state']\n",
    "    mdp.reset(state=s) # Set start state to `s`\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(T):\n",
    "        random_action = random.choice(mdp.get_possible_actions(s))\n",
    "        \n",
    "        s, r, done, _ = mdp.step(random_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    avg_rewards.append(np.mean(rewards))\n",
    "\n",
    "print(\"Average reward for random policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_data.to_csv(data_dir + 'mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "mdp_episodes.to_csv(data_dir + 'mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 4\n",
    "\n",
    "# mdp_data = pd.read_csv('./rl_data/mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "# mdp_episodes = pd.read_csv('./rl_data/mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
