{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pprint\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_PERIOD = 1\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (2385300, 40)\n"
     ]
    }
   ],
   "source": [
    "data_dir = './rl_data/train/' + str(TIME_PERIOD) + '_month_period/' \n",
    "\n",
    "mdp_data = pd.read_csv('./jmp/train/all_data-' + str(TIME_PERIOD) + '_month_period.csv')\n",
    "print('Train data:', mdp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'action', 'reward', 'bloc', 'date', 'age', 'income_bracket',\n",
       "       'Type_Cluster_3', 'Type_Cluster_5', 'Type_Cluster_10',\n",
       "       'num_gifts_to_date', 'num_promotions_to_date',\n",
       "       'frequency-gifts_per_prom', 'last_gift_amount',\n",
       "       'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
       "       'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
       "       'recent_amount_per_prom', 'months_since_last_gift',\n",
       "       'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
       "       'gift_recency_ratio', 'prom_recency_ratio',\n",
       "       'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
       "       'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
       "       'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago',\n",
       "       'Cluster 1 Components', 'Cluster 2 Components', 'Cluster 3 Components',\n",
       "       'Cluster 4 Components', 'Cluster 5 Components', 'State_Cluster_4',\n",
       "       'State_Cluster_6', 'State_Cluster_9', 'State_Cluster_11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_data['state'] = mdp_data['State_Cluster_' + str(num_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_columns 29 \n",
      "\n",
      "type_features 2\n",
      "state_features 21\n",
      "removed 6\n"
     ]
    }
   ],
   "source": [
    "all_columns = [\n",
    "    'num_gifts_to_date', 'num_promotions_to_date',\n",
    "    'frequency-gifts_per_prom', 'last_gift_amount',\n",
    "    'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
    "    'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
    "    'recent_amount_per_prom', 'months_since_last_gift',\n",
    "    'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
    "    'gift_recency_ratio', 'prom_recency_ratio',\n",
    "    'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
    "    'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
    "    'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago', \n",
    "    \n",
    "    'age', 'income_bracket', \n",
    "\n",
    "#     'type', \n",
    "    'state',\n",
    "\n",
    "    'id', 'action', \n",
    "    'reward', 'bloc', 'date'\n",
    "]\n",
    "\n",
    "type_features = [\n",
    "    'age', 'income_bracket',\n",
    "]\n",
    "\n",
    "state_features = [\n",
    "    'num_gifts_to_date', 'num_promotions_to_date',\n",
    "    'frequency-gifts_per_prom', 'last_gift_amount',\n",
    "    'total_gifts_amount_to_date', 'num_recent_proms', 'num_recent_gifts',\n",
    "    'total_recent_gifts_amount', 'recent_amount_per_gift',\n",
    "    'recent_amount_per_prom', 'months_since_last_gift',\n",
    "    'months_since_last_prom', 'months_from_first_prom_to_gift',\n",
    "    'gift_recency_ratio', 'prom_recency_ratio',\n",
    "    'did_receive_gift_1_months_ago', 'did_receive_gift_2_months_ago',\n",
    "    'did_receive_gift_3_months_ago', 'did_mail_prom_1_months_ago',\n",
    "    'did_mail_prom_2_months_ago', 'did_mail_prom_3_months_ago', \n",
    "]\n",
    "\n",
    "removed = [\n",
    "    'id', 'state', # 'type', \n",
    "    'action', 'date', 'bloc', 'reward', \n",
    "]\n",
    "\n",
    "print('all_columns', len(all_columns), '\\n')\n",
    "print('type_features', len(type_features))\n",
    "print('state_features', len(state_features))\n",
    "print('removed', len(removed))\n",
    "\n",
    "assert len(all_columns) == len(type_features) + len(state_features) + len(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE_COST = -0.68 # Cost (negative reward) for taking each action\n",
    "\n",
    "# Change actions to numbers\n",
    "ACTION_0 = 'no_action'\n",
    "ACTION_1 = 'labels_only'\n",
    "ACTION_2 = 'thank_you_with_labels'\n",
    "ACTION_3 = 'calendars_with_stickers'\n",
    "ACTION_4 = 'blank_cards_with_labels'\n",
    "ACTION_5 = 'greeting_cards_with_labels'\n",
    "ACTION_6 = 'labels_and_notepad'\n",
    "\n",
    "ACTIONS = {\n",
    "    ACTION_0: 0,\n",
    "    ACTION_1: 1,\n",
    "    ACTION_2: 2,\n",
    "    ACTION_3: 3,\n",
    "    ACTION_4: 4,\n",
    "    ACTION_5: 5,\n",
    "    ACTION_6: 6,\n",
    "}\n",
    "\n",
    "ACTION_NAMES = {\n",
    "    0: ACTION_0,\n",
    "    1: ACTION_1,\n",
    "    2: ACTION_2,\n",
    "    3: ACTION_3,\n",
    "    4: ACTION_4,\n",
    "    5: ACTION_5,\n",
    "    6: ACTION_6,\n",
    "}\n",
    "\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_0, 'action'] = ACTIONS[ACTION_0]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_1, 'action'] = ACTIONS[ACTION_1]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_2, 'action'] = ACTIONS[ACTION_2]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_3, 'action'] = ACTIONS[ACTION_3]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_4, 'action'] = ACTIONS[ACTION_4]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_5, 'action'] = ACTIONS[ACTION_5]\n",
    "mdp_data.loc[mdp_data['action'] == ACTION_6, 'action'] = ACTIONS[ACTION_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>Type_Cluster_3</th>\n",
       "      <th>Type_Cluster_5</th>\n",
       "      <th>Type_Cluster_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Cluster 1 Components</th>\n",
       "      <th>Cluster 2 Components</th>\n",
       "      <th>Cluster 3 Components</th>\n",
       "      <th>Cluster 4 Components</th>\n",
       "      <th>Cluster 5 Components</th>\n",
       "      <th>State_Cluster_4</th>\n",
       "      <th>State_Cluster_6</th>\n",
       "      <th>State_Cluster_9</th>\n",
       "      <th>State_Cluster_11</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95515</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>09-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.940385</td>\n",
       "      <td>3.094768</td>\n",
       "      <td>-0.681861</td>\n",
       "      <td>2.081938</td>\n",
       "      <td>0.598063</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>10-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.082934</td>\n",
       "      <td>-0.654150</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>12-1994</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.082934</td>\n",
       "      <td>-0.450526</td>\n",
       "      <td>-0.487073</td>\n",
       "      <td>0.547664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>02-1995</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.072428</td>\n",
       "      <td>-0.598728</td>\n",
       "      <td>-0.400118</td>\n",
       "      <td>0.522465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95515</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>04-1995</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>3.063180</td>\n",
       "      <td>-0.571017</td>\n",
       "      <td>0.767771</td>\n",
       "      <td>0.497266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385295</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>12-1997</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.427811</td>\n",
       "      <td>-0.430789</td>\n",
       "      <td>6.379600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385296</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>02-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.631435</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.354401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385297</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>04-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>0.835059</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.329202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385298</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>06-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>1.038683</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>6.304003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385299</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>08-1998</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>18.638946</td>\n",
       "      <td>1.242307</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>6.278803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2385300 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc     date  age  income_bracket  \\\n",
       "0         95515      5    0.00     0  09-1994   60               3   \n",
       "1         95515      6   -0.68     1  10-1994   60               3   \n",
       "2         95515      0    0.00     2  12-1994   60               3   \n",
       "3         95515      6   -0.68     3  02-1995   60               3   \n",
       "4         95515      2   -0.68     4  04-1995   60               3   \n",
       "...         ...    ...     ...   ...      ...  ...             ...   \n",
       "2385295  185114      0    0.00    20  12-1997   80               5   \n",
       "2385296  185114      0    0.00    21  02-1998   80               5   \n",
       "2385297  185114      0    0.00    22  04-1998   80               5   \n",
       "2385298  185114      0    0.00    23  06-1998   80               5   \n",
       "2385299  185114      0    0.00    24  08-1998   80               5   \n",
       "\n",
       "         Type_Cluster_3  Type_Cluster_5  Type_Cluster_10  ...  \\\n",
       "0                     1               1                1  ...   \n",
       "1                     1               1                1  ...   \n",
       "2                     1               1                1  ...   \n",
       "3                     1               1                1  ...   \n",
       "4                     1               1                1  ...   \n",
       "...                 ...             ...              ...  ...   \n",
       "2385295               2               4                8  ...   \n",
       "2385296               2               4                8  ...   \n",
       "2385297               2               4                8  ...   \n",
       "2385298               2               4                8  ...   \n",
       "2385299               2               4                8  ...   \n",
       "\n",
       "         Cluster 1 Components  Cluster 2 Components  Cluster 3 Components  \\\n",
       "0                    3.940385              3.094768             -0.681861   \n",
       "1                   -0.926485              3.082934             -0.654150   \n",
       "2                   -0.926485              3.082934             -0.450526   \n",
       "3                   -0.926485              3.072428             -0.598728   \n",
       "4                   -0.926485              3.063180             -0.571017   \n",
       "...                       ...                   ...                   ...   \n",
       "2385295             -0.926485             18.638946              0.427811   \n",
       "2385296             -0.926485             18.638946              0.631435   \n",
       "2385297             -0.926485             18.638946              0.835059   \n",
       "2385298             -0.926485             18.638946              1.038683   \n",
       "2385299             -0.926485             18.638946              1.242307   \n",
       "\n",
       "         Cluster 4 Components  Cluster 5 Components  State_Cluster_4  \\\n",
       "0                    2.081938              0.598063                3   \n",
       "1                   -1.685633              0.572864                0   \n",
       "2                   -0.487073              0.547664                0   \n",
       "3                   -0.400118              0.522465                0   \n",
       "4                    0.767771              0.497266                0   \n",
       "...                       ...                   ...              ...   \n",
       "2385295             -0.430789              6.379600                0   \n",
       "2385296             -1.368910              6.354401                0   \n",
       "2385297             -1.368910              6.329202                0   \n",
       "2385298             -1.368910              6.304003                0   \n",
       "2385299             -1.685633              6.278803                0   \n",
       "\n",
       "         State_Cluster_6  State_Cluster_9  State_Cluster_11  state  \n",
       "0                      5                5                 7      3  \n",
       "1                      0                0                 1      0  \n",
       "2                      0                0                 1      0  \n",
       "3                      0                0                 1      0  \n",
       "4                      0                0                 1      0  \n",
       "...                  ...              ...               ...    ...  \n",
       "2385295                0                5                 1      0  \n",
       "2385296                0                0                 1      0  \n",
       "2385297                0                0                 1      0  \n",
       "2385298                0                0                 1      0  \n",
       "2385299                0                0                 1      0  \n",
       "\n",
       "[2385300 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mdp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states: 4\n",
      "num_actions: 7\n",
      "========================== Processed 0 rows ( 0.0 %)========================== 0.0564763069152832 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 3.2452666560808816 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 6.482110134760538 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 9.654932022094727 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 13.156391878922781 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 16.481238941351574 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 18.904829998811085 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 21.51179946263631 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 23.933282840251923 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 26.40637547969818 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 28.808360532919565 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 31.204703946908314 mins\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_states = max(mdp_data['state']) + 1 # 0 based index\n",
    "num_actions = max(mdp_data['action']) + 1 # 0 based index\n",
    "\n",
    "print('num_states:', num_states)\n",
    "print('num_actions:', num_actions)\n",
    "\n",
    "# transitions[curr_state][action][next_state]\n",
    "transition_counts = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# transitions[curr_state][action]\n",
    "transition_sums = np.zeros((num_states, num_actions))\n",
    "\n",
    "# rewards[curr_state][action][next_state]\n",
    "rewards = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for i, row in mdp_data.iterrows():\n",
    "    if i + 1 >= len(mdp_data) or mdp_data.iloc[i + 1]['bloc'] == 0:\n",
    "        # Skip since there is no transition to make\n",
    "        continue\n",
    "\n",
    "    curr_s = row['state']\n",
    "    action = row['action']\n",
    "    next_s = mdp_data.iloc[i + 1]['state']\n",
    "    reward = mdp_data.iloc[i + 1]['reward']\n",
    "\n",
    "    transition_counts[curr_s, action, next_s] += 1\n",
    "    rewards[curr_s, action, next_s] += reward\n",
    "\n",
    "    transition_sums[curr_s, action] += 1\n",
    "\n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(mdp_data) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "        \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transition_counts, open(data_dir + 'transition_counts_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(transition_sums, open(data_dir + 'transition_sums_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 7\n",
    "\n",
    "# transition_counts = pickle.load(open(data_dir + 'transition_counts_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "# transition_sums = pickle.load(open(data_dir + 'transition_sums_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "# Rare transition threshold\n",
    "# TRANSITION_THRESHOLD = 5\n",
    "\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        if transition_sums[s][a] == 0:\n",
    "            transitions[s][a][:] = 0\n",
    "            \n",
    "            assert np.isclose(np.sum(rewards[s][a][:]), 0)\n",
    "        else:\n",
    "            transitions[s][a][:] = transition_counts[s][a][:] / transition_sums[s][a]\n",
    "            rewards[s][a][:] = rewards[s][a][:] / transition_sums[s][a]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transitions, open(data_dir + 'transitions_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(rewards, open(data_dir + 'rewards_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 7\n",
    "\n",
    "# transitions = pickle.load(open(data_dir + 'transitions_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "# rewards = pickle.load(open(data_dir + 'rewards_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning on MDP\n",
    "\n",
    "https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919\n",
    "\n",
    "https://www.datahubbs.com/reinforcement-learning-markov-decision-processes/  \n",
    "We can set this up just like we did for the MDP example above in order to solve for ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
    "\n",
    "\"\"\"\n",
    "Implements the following:\n",
    "- MDP class\n",
    "- plot_graph \n",
    "- plot_graph_with_state_values\n",
    "- plot_graph_optimal_strategy_and_state_values\n",
    "\"\"\"\n",
    "\n",
    "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self, state=None):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if state:\n",
    "            self._current_state = state\n",
    "            return self._current_state\n",
    "        \n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        state_node = str(state_node)\n",
    "        \n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            posible_action = str(posible_action)\n",
    "            \n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    try:\n",
    "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
    "                    action in next_actions]\n",
    "        optimal_action = next_actions[np.argmax(q_values)]\n",
    "    except NameError:\n",
    "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
    "        \n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp,\n",
    "                                                     state_values,\n",
    "                                                     state_node,\n",
    "                                                     gamma):\n",
    "                graph.edge(str(state_node), str(state_node) + \"-\" + str(action),\n",
    "                           **opt_s_a_edge_attrs)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      action (string) : action that is taken \n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      q (float) : Returns action-value: Expected return starting from state s, \n",
    "                  taking action a, and then following policy π\n",
    "    \"\"\"\n",
    "    \n",
    "    q = 0\n",
    "    next_states = mdp.get_next_states(state,action)\n",
    "    \n",
    "    for next_state in next_states:\n",
    "        probability = next_states[next_state]\n",
    "        reward = mdp.get_reward(state,action,next_state)\n",
    "        \n",
    "        q += probability * (reward + gamma * state_values[next_state])\n",
    "    \n",
    "    return q\n",
    "\n",
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      v (float) : Returns next state value: Expected return for next iteration, \n",
    "                  starting from state s, and then following policy π\n",
    "    \"\"\"\n",
    "\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    \n",
    "    v = 0\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if action_value > v:\n",
    "            v = action_value\n",
    "\n",
    "    return v\n",
    "\n",
    "def value_iteration(mdp, gamma, num_iter, min_difference, state_values=None):\n",
    "    \"\"\"\n",
    "    Performs num_iter value iteration steps starting from state_values.\n",
    "\n",
    "    Args:\n",
    "      mdp : the MDP that we're using\n",
    "      gamma (float) : discount factor for MDP\n",
    "      num_iter (string) : maximum number of iterations\n",
    "      min_difference (float) : stop Value Iteration if new values are at least this close to old values\n",
    "      state_values (dict- {string->float}) : state-values for the states\n",
    "\n",
    "    Returns:\n",
    "      state_values (dict) : Returns the state-values at the end of Value Iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize V(s) to 0 if argument is not provided\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "    new_state_values = {}\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        for s in state_values:\n",
    "            new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        state_values = dict(new_state_values)\n",
    "\n",
    "        if i % (0.1 * num_iter) == 0:\n",
    "            print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "            print(\"  \".join(\"V(%s) = %.3f\" % (s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if diff < min_difference:\n",
    "            print(\"Terminated\")\n",
    "            break\n",
    "            \n",
    "    return state_values\n",
    "\n",
    "def get_optimal_action(mdp, state_values, state, gamma=1, debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      mdp (MDP): the MDP that we're using\n",
    "      state_values (dict- {string->float}) : state-values for the states in the current iteration\n",
    "      state (string) : start state\n",
    "      gamma (float) : discount factor\n",
    "\n",
    "    Returns:\n",
    "      optimal_action (string) : Returns the optimal action - action that results \n",
    "                                in the maximum action-value.\n",
    "    \"\"\"\n",
    "\n",
    "    if mdp.is_terminal(state): \n",
    "        return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    # Compute optimal action as per formula above. \n",
    "    optimal_action = None\n",
    "    optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "    if debug:\n",
    "        print()\n",
    "    \n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "\n",
    "        if action_value >= optimal_action_value:\n",
    "            optimal_action_value = action_value\n",
    "            optimal_action = action\n",
    "            \n",
    "            if debug:\n",
    "                print('action', action, ':', action_value)\n",
    "\n",
    "    return optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_dict = {}\n",
    "rewards_dict = {}\n",
    "\n",
    "for curr_s in range(transitions.shape[0]):\n",
    "    transitions_dict[curr_s] = {}\n",
    "    rewards_dict[curr_s] = {}\n",
    "    for a in range(transitions.shape[1]):        \n",
    "        if math.isclose(sum(transitions[curr_s][a][:]), 0):\n",
    "            continue\n",
    "        \n",
    "        transitions_dict[curr_s][a] = {}\n",
    "        rewards_dict[curr_s][a] = {}\n",
    "        for next_s in range(transitions.shape[2]):\n",
    "            transitions_dict[curr_s][a][next_s] = transitions[curr_s][a][next_s].item()\n",
    "            rewards_dict[curr_s][a][next_s] = rewards[curr_s][a][next_s].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions:\n",
      "{0: {0: {0: 0.968902794433911,\n",
      "         1: 0.00018625092957972946,\n",
      "         2: 5.49373245522943e-05,\n",
      "         3: 0.030856017311956908},\n",
      "     1: {0: 0.22882882882882882,\n",
      "         1: 0.0036036036036036037,\n",
      "         2: 0.0,\n",
      "         3: 0.7675675675675676},\n",
      "     2: {0: 0.42772259666548423,\n",
      "         1: 0.003458673288400142,\n",
      "         2: 0.0,\n",
      "         3: 0.5688187300461156},\n",
      "     3: {0: 0.5069444444444444,\n",
      "         1: 0.006944444444444444,\n",
      "         2: 0.0,\n",
      "         3: 0.4861111111111111},\n",
      "     4: {0: 0.8313997465544172,\n",
      "         1: 0.001385109580832441,\n",
      "         2: 0.0,\n",
      "         3: 0.16721514386475045},\n",
      "     5: {0: 0.5988861386138614,\n",
      "         1: 0.0013613861386138613,\n",
      "         2: 0.0,\n",
      "         3: 0.3997524752475248},\n",
      "     6: {0: 0.5456280887049294,\n",
      "         1: 0.0038179515084476637,\n",
      "         2: 0.0,\n",
      "         3: 0.4505539597866229}},\n",
      " 1: {0: {0: 0.03815496126477385,\n",
      "         1: 0.9570128913818787,\n",
      "         2: 0.0021785383432928506,\n",
      "         3: 0.0026536090100545313},\n",
      "     1: {0: 0.024422012373819604,\n",
      "         1: 0.7632692933897753,\n",
      "         2: 0.0,\n",
      "         3: 0.21230869423640508},\n",
      "     2: {0: 0.025794564716720404,\n",
      "         1: 0.9583909104867189,\n",
      "         2: 0.0,\n",
      "         3: 0.015814524796560724},\n",
      "     3: {0: 0.483401281304601, 1: 0.516598718695399, 2: 0.0, 3: 0.0},\n",
      "     4: {0: 0.014394869695729295,\n",
      "         1: 0.974553145040251,\n",
      "         2: 0.0,\n",
      "         3: 0.011051985264019648},\n",
      "     5: {0: 0.11979578436758814,\n",
      "         1: 0.8684122368579736,\n",
      "         2: 0.0,\n",
      "         3: 0.011791978774438206},\n",
      "     6: {0: 0.1783808807360685,\n",
      "         1: 0.7693536253689023,\n",
      "         2: 0.0,\n",
      "         3: 0.05226549389502922}},\n",
      " 2: {0: {0: 0.04872688287322434,\n",
      "         1: 0.11475029036004646,\n",
      "         2: 0.8291342803537926,\n",
      "         3: 0.007388546412936657}},\n",
      " 3: {0: {0: 0.28173395144380164,\n",
      "         1: 0.0015158022383346386,\n",
      "         2: 0.0,\n",
      "         3: 0.7167502463178638},\n",
      "     1: {0: 0.00101010101010101,\n",
      "         1: 8.417508417508418e-05,\n",
      "         2: 0.0,\n",
      "         3: 0.9989057239057239},\n",
      "     2: {0: 0.009522259298445511,\n",
      "         1: 2.7128943870215132e-05,\n",
      "         2: 0.0,\n",
      "         3: 0.9904506117576842},\n",
      "     3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.0},\n",
      "     4: {0: 0.0032631642439230352, 1: 0.0, 2: 0.0, 3: 0.996736835756077},\n",
      "     5: {0: 0.33835718232570544, 1: 0.0, 2: 0.0, 3: 0.6616428176742946},\n",
      "     6: {0: 0.01450890093837431,\n",
      "         1: 4.951843323677239e-05,\n",
      "         2: 0.0,\n",
      "         3: 0.9854415806283889}}}\n",
      "\n",
      "Rewards:\n",
      "{0: {0: {0: 0.02836456944543648,\n",
      "         1: -4.555778133604894e-06,\n",
      "         2: 0.0,\n",
      "         3: 0.013996167786625841},\n",
      "     1: {0: 0.5384864864864862, 1: 0.0, 2: 0.0, 3: 2.680738738738749},\n",
      "     2: {0: 0.9044359702021416,\n",
      "         1: -0.0023518978361120957,\n",
      "         2: 0.0,\n",
      "         3: 0.7611750620786796},\n",
      "     3: {0: 0.10611111111111116,\n",
      "         1: -0.004722222222222222,\n",
      "         2: 0.0,\n",
      "         3: 0.9616666666666652},\n",
      "     4: {0: 0.04904299733783956,\n",
      "         1: -0.00018035894967435186,\n",
      "         2: 0.0,\n",
      "         3: 0.09315009283180133},\n",
      "     5: {0: 0.29363407590756646,\n",
      "         1: -0.0006171617161716171,\n",
      "         2: 0.0,\n",
      "         3: 0.30501815181515124},\n",
      "     6: {0: 1.6264851652962307,\n",
      "         1: -0.001334498938466755,\n",
      "         2: 0.0,\n",
      "         3: 0.9553658275498298}},\n",
      " 1: {0: {0: 0.5320661841306876,\n",
      "         1: -0.18614951831234858,\n",
      "         2: 0.0,\n",
      "         3: 0.0640001289477519},\n",
      "     1: {0: 0.41309019863236734,\n",
      "         1: -0.10207749918593369,\n",
      "         2: 0.0,\n",
      "         3: 4.332191468577009},\n",
      "     2: {0: 0.4620697067403644,\n",
      "         1: -0.6475295562721806,\n",
      "         2: 0.0,\n",
      "         3: 0.41322585598034633},\n",
      "     3: {0: 4.298520675596971, 1: -0.06178217821782191, 2: 0.0, 3: 0.0},\n",
      "     4: {0: 0.2291758766543858,\n",
      "         1: -0.18042638832041114,\n",
      "         2: 0.0,\n",
      "         3: 0.14007231545913446},\n",
      "     5: {0: 1.5997905583771965,\n",
      "         1: -0.5049679070579531,\n",
      "         2: 0.0,\n",
      "         3: 0.20733528079649263},\n",
      "     6: {0: 2.7549299230370865,\n",
      "         1: -0.2404259012789284,\n",
      "         2: 0.0,\n",
      "         3: 1.0201884150222833}},\n",
      " 2: {0: {0: 0.4075658893951456,\n",
      "         1: -0.07818207808453101,\n",
      "         2: 0.5607856696149379,\n",
      "         3: 0.0017932636469221565}},\n",
      " 3: {0: {0: 0.054412689791065186,\n",
      "         1: -0.00022118080994366275,\n",
      "         2: 0.0,\n",
      "         3: 0.3075516951727126},\n",
      "     1: {0: 0.007067340067340068, 1: 0.0, 2: 0.0, 3: 3.928615319865294},\n",
      "     2: {0: -0.0036070643769838375,\n",
      "         1: -1.844768183174629e-05,\n",
      "         2: 0.0,\n",
      "         3: 0.8163050378446378},\n",
      "     3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 1.6000000000000003},\n",
      "     4: {0: -0.001062549007780938, 1: 0.0, 2: 0.0, 3: 0.18749170637543042},\n",
      "     5: {0: 0.2924413619647739, 1: 0.0, 2: 0.0, 3: 0.6136455614197124},\n",
      "     6: {0: 0.00472108742479379,\n",
      "         1: -4.48967128013403e-05,\n",
      "         2: 0.0,\n",
      "         3: 1.0772852344290351}}}\n"
     ]
    }
   ],
   "source": [
    "print('Transitions:')\n",
    "pprint.pprint(transitions_dict)\n",
    "\n",
    "print('\\nRewards:')\n",
    "pprint.pprint(rewards_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run KDD Marketing RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = np.arange(num_states)\n",
    "mdp = MDP(transitions_dict, rewards_dict, initial_state=random.choice(initial_states), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.92432   |   V(0) = 2.181  V(1) = 2.046  V(2) = 0.476  V(3) = 3.924\n",
      "\n",
      "iter  100   |   diff: 0.02322   |   V(0) = 75.744  V(1) = 72.281  V(2) = 58.341  V(3) = 77.993\n",
      "\n",
      "iter  200   |   diff: 0.00014   |   V(0) = 76.182  V(1) = 72.720  V(2) = 58.779  V(3) = 78.432\n",
      "\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "mdp.reset()\n",
    "\n",
    "# Discount factor γ / gamma\n",
    "# If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward. \n",
    "# If γ=1, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
    "\n",
    "gamma = 0.95\n",
    "num_iter = 1000\n",
    "min_difference = 1e-5\n",
    "\n",
    "state_values = value_iteration(mdp, gamma, num_iter, min_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MDP state values after running Value Iteration:\n",
      "{0: 76.18460268783707,\n",
      " 1: 72.7220392495309,\n",
      " 2: 58.78160738327181,\n",
      " 3: 78.43398282501718}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final MDP state values after running Value Iteration:\")\n",
    "pprint.pprint(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action for each MDP state\n",
      "\n",
      "action 0 : 72.4677025653448\n",
      "action 1 : 76.18461175609477\n",
      "    MDP State 0 : optimal action  1 labels_only\n",
      "\n",
      "action 0 : 69.03931699405608\n",
      "action 1 : 71.1702702209369\n",
      "action 3 : 72.72204831778859\n",
      "    MDP State 1 : optimal action  3 calendars_with_stickers\n",
      "\n",
      "action 0 : 58.78161645152951\n",
      "    MDP State 2 : optimal action  0 no_action\n",
      "\n",
      "action 0 : 74.13778531923028\n",
      "action 1 : 78.43399189327488\n",
      "    MDP State 3 : optimal action  1 labels_only\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal action for each MDP state\")\n",
    "\n",
    "for state in range(num_states):\n",
    "    value = state_values[state]\n",
    "    optimal_action_id = get_optimal_action(mdp, state_values, state, gamma, debug=True)\n",
    "    print(\"    MDP State\", state, \": optimal action \", optimal_action_id, ACTION_NAMES[optimal_action_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(transitions_dict, open(data_dir + 'mdp_transitions_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(rewards_dict, open(data_dir + 'mdp_rewards_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))\n",
    "pickle.dump(state_values, open(data_dir + 'mdp_state_values_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 25 # total num months / TIME_PERIOD of each event\n",
    "\n",
    "def get_average_reward(start_state, mdp, state_values, gamma):\n",
    "    \"\"\"\n",
    "    Returns the average reward for the episode of length EPISODE_LENGTH \n",
    "    starting at `start_state`, when following the OPTIMAL policy. \n",
    "    \"\"\"\n",
    "    s = start_state\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "    \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10000 episodes: 3.922005379194813\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward over random episodes\n",
    "num_samples = 10000\n",
    "\n",
    "avg_rewards = []\n",
    "for _ in range(num_samples):\n",
    "    start_s = mdp.reset() # select random start state\n",
    "\n",
    "    r = get_average_reward(start_s, mdp, state_values, gamma)\n",
    "    avg_rewards.append(r)\n",
    "\n",
    "print(\"Average reward over\", num_samples, \"episodes:\", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 4\n",
    "\n",
    "# transitions_dict = pickle.load(open('./mdp_data/transitions_dict' + str(num_states) + '.p', \"rb\"))\n",
    "# rewards_dict = pickle.load(open('./mdp_data/rewards_dict' + str(num_states) + '.p', \"rb\"))\n",
    "# state_values = pickle.load(open('./mdp_data/state_values' + str(num_states) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95515</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95515</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95515</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385295</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385296</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385297</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385298</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385299</th>\n",
       "      <td>185114</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2385300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc  state\n",
       "0         95515      5    0.00     0      3\n",
       "1         95515      6   -0.68     1      0\n",
       "2         95515      0    0.00     2      0\n",
       "3         95515      6   -0.68     3      0\n",
       "4         95515      2   -0.68     4      0\n",
       "...         ...    ...     ...   ...    ...\n",
       "2385295  185114      0    0.00    20      0\n",
       "2385296  185114      0    0.00    21      0\n",
       "2385297  185114      0    0.00    22      0\n",
       "2385298  185114      0    0.00    23      0\n",
       "2385299  185114      0    0.00    24      0\n",
       "\n",
       "[2385300 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdp_episodes = mdp_data[['id', 'action', 'reward', 'bloc', 'state']]\n",
    "\n",
    "sample_episodes = mdp_episodes\n",
    "display(sample_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for original dataset policy:  0.5396969018572084\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following original data policy\n",
    "\n",
    "print(\"Average reward for original dataset policy: \", np.mean(sample_episodes['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 0.01663888692855835 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 0.6408277869224548 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 1.2518868843714397 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 1.8551175753275553 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 2.5297335704167683 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 3.170107130209605 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 3.80388784011205 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 4.4368154843648275 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 5.041568636894226 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 5.655233319600423 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 6.262084102630615 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 6.876586981614431 mins\n",
      "Average reward for optimal RL policy:  3.6046972442456138\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following RL optimal policy\n",
    "start_time = time.time()\n",
    "\n",
    "avg_rewards = []\n",
    "for i, row in sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    s = row['state']\n",
    "    mdp.reset(s) # Set start state to `s`\n",
    "\n",
    "    r = get_average_reward(s, mdp, state_values, gamma)\n",
    "    avg_rewards.append(r)\n",
    "\n",
    "print(\"Average reward for optimal RL policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 0.006732459863026937 mins\n",
      "========================== Processed 200000 rows ( 8.384689556869157 %)========================== 0.4966660261154175 mins\n",
      "========================== Processed 400000 rows ( 16.769379113738314 %)========================== 0.9474374294281006 mins\n",
      "========================== Processed 600000 rows ( 25.154068670607472 %)========================== 1.420689543088277 mins\n",
      "========================== Processed 800000 rows ( 33.53875822747663 %)========================== 1.8668601592381795 mins\n",
      "========================== Processed 1000000 rows ( 41.92344778434578 %)========================== 2.3381288409233094 mins\n",
      "========================== Processed 1200000 rows ( 50.308137341214945 %)========================== 2.7889917929967245 mins\n",
      "========================== Processed 1400000 rows ( 58.69282689808409 %)========================== 3.2523974339167276 mins\n",
      "========================== Processed 1600000 rows ( 67.07751645495325 %)========================== 3.7054800629615783 mins\n",
      "========================== Processed 1800000 rows ( 75.4622060118224 %)========================== 4.15553518931071 mins\n",
      "========================== Processed 2000000 rows ( 83.84689556869156 %)========================== 4.644352054595947 mins\n",
      "========================== Processed 2200000 rows ( 92.23158512556073 %)========================== 5.106337567170461 mins\n",
      "Average reward for random policy:  0.9424275645978695\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following random policy\n",
    "start_time = time.time()\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for i, row in sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    \n",
    "    s = row['state']\n",
    "    mdp.reset(state=s) # Set start state to `s`\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        random_action = random.choice(mdp.get_possible_actions(s))\n",
    "        \n",
    "        s, r, done, _ = mdp.step(random_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    avg_rewards.append(np.mean(rewards))\n",
    "\n",
    "print(\"Average reward for random policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_data.to_csv(data_dir + 'mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "mdp_episodes.to_csv(data_dir + 'mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 4\n",
    "\n",
    "# mdp_data = pd.read_csv(data_dir + 'mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "# mdp_episodes = pd.read_csv(data_dir + 'mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: (2409175, 40)\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = './rl_data/test/' + str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "test_mdp_data = pd.read_csv('./jmp/test/all_data-' + str(TIME_PERIOD) + '_month_period.csv')\n",
    "print('Test data:', test_mdp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mdp_data['state'] = test_mdp_data['State_Cluster_' + str(num_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE_COST = -0.68 # Cost (negative reward) for taking each action\n",
    "# FIXME: different costs for different actions\n",
    "\n",
    "# Change actions to numbers\n",
    "ACTION_0 = 'no_action'\n",
    "ACTION_1 = 'labels_only'\n",
    "ACTION_2 = 'thank_you_with_labels'\n",
    "ACTION_3 = 'calendars_with_stickers'\n",
    "ACTION_4 = 'blank_cards_with_labels'\n",
    "ACTION_5 = 'greeting_cards_with_labels'\n",
    "ACTION_6 = 'labels_and_notepad'\n",
    "\n",
    "ACTIONS = {\n",
    "    ACTION_0: 0,\n",
    "    ACTION_1: 1,\n",
    "    ACTION_2: 2,\n",
    "    ACTION_3: 3,\n",
    "    ACTION_4: 4,\n",
    "    ACTION_5: 5,\n",
    "    ACTION_6: 6,\n",
    "}\n",
    "\n",
    "ACTION_NAMES = {\n",
    "    0: ACTION_0,\n",
    "    1: ACTION_1,\n",
    "    2: ACTION_2,\n",
    "    3: ACTION_3,\n",
    "    4: ACTION_4,\n",
    "    5: ACTION_5,\n",
    "    6: ACTION_6,\n",
    "}\n",
    "\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_0, 'action'] = ACTIONS[ACTION_0]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_1, 'action'] = ACTIONS[ACTION_1]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_2, 'action'] = ACTIONS[ACTION_2]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_3, 'action'] = ACTIONS[ACTION_3]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_4, 'action'] = ACTIONS[ACTION_4]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_5, 'action'] = ACTIONS[ACTION_5]\n",
    "test_mdp_data.loc[test_mdp_data['action'] == ACTION_6, 'action'] = ACTIONS[ACTION_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>date</th>\n",
       "      <th>age</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>Type_Cluster_3</th>\n",
       "      <th>Type_Cluster_5</th>\n",
       "      <th>Type_Cluster_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Cluster 1 Components</th>\n",
       "      <th>Cluster 2 Components</th>\n",
       "      <th>Cluster 3 Components</th>\n",
       "      <th>Cluster 4 Components</th>\n",
       "      <th>Cluster 5 Components</th>\n",
       "      <th>State_Cluster_4</th>\n",
       "      <th>State_Cluster_6</th>\n",
       "      <th>State_Cluster_9</th>\n",
       "      <th>State_Cluster_11</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188946</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>09-1994</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>-0.739844</td>\n",
       "      <td>-0.501068</td>\n",
       "      <td>0.854727</td>\n",
       "      <td>-1.800008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188946</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>10-1994</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>-0.755040</td>\n",
       "      <td>-0.501068</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>-1.800008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188946</td>\n",
       "      <td>0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2</td>\n",
       "      <td>12-1994</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>-0.393034</td>\n",
       "      <td>-0.737508</td>\n",
       "      <td>-0.487073</td>\n",
       "      <td>1.342906</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188946</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>02-1995</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.648679</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.828984</td>\n",
       "      <td>-0.400118</td>\n",
       "      <td>1.317706</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188946</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>04-1995</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4.392178</td>\n",
       "      <td>-0.430255</td>\n",
       "      <td>-0.824365</td>\n",
       "      <td>0.767771</td>\n",
       "      <td>1.292507</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409170</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>12-1997</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>0.359419</td>\n",
       "      <td>-0.592593</td>\n",
       "      <td>-0.430789</td>\n",
       "      <td>1.045474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409171</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>02-1998</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>0.359419</td>\n",
       "      <td>-0.498741</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>1.020275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409172</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>04-1998</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>0.359419</td>\n",
       "      <td>-0.404888</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>0.995076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409173</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>06-1998</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>0.359419</td>\n",
       "      <td>-0.311035</td>\n",
       "      <td>-1.368910</td>\n",
       "      <td>0.969877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409174</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>08-1998</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926485</td>\n",
       "      <td>0.359419</td>\n",
       "      <td>-0.217183</td>\n",
       "      <td>-1.685633</td>\n",
       "      <td>0.944677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2409175 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc     date  age  income_bracket  \\\n",
       "0        188946      5    0.00     0  09-1994   81               1   \n",
       "1        188946      6   -0.68     1  10-1994   81               1   \n",
       "2        188946      0   24.00     2  12-1994   81               1   \n",
       "3        188946      6   -0.68     3  02-1995   81               1   \n",
       "4        188946      2   -0.68     4  04-1995   81               1   \n",
       "...         ...    ...     ...   ...      ...  ...             ...   \n",
       "2409170  123497      0    0.00    20  12-1997   35               3   \n",
       "2409171  123497      0    0.00    21  02-1998   35               3   \n",
       "2409172  123497      0    0.00    22  04-1998   35               3   \n",
       "2409173  123497      0    0.00    23  06-1998   35               3   \n",
       "2409174  123497      0    0.00    24  08-1998   35               3   \n",
       "\n",
       "         Type_Cluster_3  Type_Cluster_5  Type_Cluster_10  ...  \\\n",
       "0                     1               2                3  ...   \n",
       "1                     1               2                3  ...   \n",
       "2                     1               2                3  ...   \n",
       "3                     1               2                3  ...   \n",
       "4                     1               2                3  ...   \n",
       "...                 ...             ...              ...  ...   \n",
       "2409170               3               5               10  ...   \n",
       "2409171               3               5               10  ...   \n",
       "2409172               3               5               10  ...   \n",
       "2409173               3               5               10  ...   \n",
       "2409174               3               5               10  ...   \n",
       "\n",
       "         Cluster 1 Components  Cluster 2 Components  Cluster 3 Components  \\\n",
       "0                   -0.926485             -0.739844             -0.501068   \n",
       "1                   -0.926485             -0.755040             -0.501068   \n",
       "2                   -0.926485             -0.393034             -0.737508   \n",
       "3                    5.648679             -0.413584             -0.828984   \n",
       "4                    4.392178             -0.430255             -0.824365   \n",
       "...                       ...                   ...                   ...   \n",
       "2409170             -0.926485              0.359419             -0.592593   \n",
       "2409171             -0.926485              0.359419             -0.498741   \n",
       "2409172             -0.926485              0.359419             -0.404888   \n",
       "2409173             -0.926485              0.359419             -0.311035   \n",
       "2409174             -0.926485              0.359419             -0.217183   \n",
       "\n",
       "         Cluster 4 Components  Cluster 5 Components  State_Cluster_4  \\\n",
       "0                    0.854727             -1.800008                1   \n",
       "1                   -1.685633             -1.800008                1   \n",
       "2                   -0.487073              1.342906                0   \n",
       "3                   -0.400118              1.317706                3   \n",
       "4                    0.767771              1.292507                3   \n",
       "...                       ...                   ...              ...   \n",
       "2409170             -0.430789              1.045474                0   \n",
       "2409171             -1.368910              1.020275                0   \n",
       "2409172             -1.368910              0.995076                0   \n",
       "2409173             -1.368910              0.969877                0   \n",
       "2409174             -1.685633              0.944677                0   \n",
       "\n",
       "         State_Cluster_6  State_Cluster_9  State_Cluster_11  state  \n",
       "0                      2                3                 4      1  \n",
       "1                      2                3                 5      1  \n",
       "2                      1                2                 3      0  \n",
       "3                      5                9                11      3  \n",
       "4                      5                9                11      3  \n",
       "...                  ...              ...               ...    ...  \n",
       "2409170                0                1                 1      0  \n",
       "2409171                0                1                 1      0  \n",
       "2409172                0                1                 1      0  \n",
       "2409173                0                1                 1      0  \n",
       "2409174                0                1                 1      0  \n",
       "\n",
       "[2409175 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_mdp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RL policy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>bloc</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188946</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188946</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188946</td>\n",
       "      <td>0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188946</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188946</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409170</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409171</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409172</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409173</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409174</th>\n",
       "      <td>123497</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2409175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id action  reward  bloc  state\n",
       "0        188946      5    0.00     0      1\n",
       "1        188946      6   -0.68     1      1\n",
       "2        188946      0   24.00     2      0\n",
       "3        188946      6   -0.68     3      3\n",
       "4        188946      2   -0.68     4      3\n",
       "...         ...    ...     ...   ...    ...\n",
       "2409170  123497      0    0.00    20      0\n",
       "2409171  123497      0    0.00    21      0\n",
       "2409172  123497      0    0.00    22      0\n",
       "2409173  123497      0    0.00    23      0\n",
       "2409174  123497      0    0.00    24      0\n",
       "\n",
       "[2409175 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_mdp_episodes = test_mdp_data[['id', 'action', 'reward', 'bloc', 'state']]\n",
    "\n",
    "test_sample_episodes = test_mdp_episodes\n",
    "display(test_sample_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num states = 4\n",
      "Num actions = 7\n"
     ]
    }
   ],
   "source": [
    "print('Num states =', num_states)\n",
    "print('Num actions =', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_dict = pickle.load(open(data_dir + 'mdp_transitions_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "rewards_dict = pickle.load(open(data_dir + 'mdp_rewards_dict_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))\n",
    "state_values = pickle.load(open(data_dir + 'mdp_state_values_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = np.arange(num_states)\n",
    "mdp = MDP(transitions_dict, rewards_dict, initial_state=random.choice(initial_states), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average reward for original dataset policy:  0.5396465678084823\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following original data policy\n",
    "\n",
    "print(\"[Test set] Average reward for original dataset policy: \", np.mean(test_sample_episodes['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 0.005672359466552734 mins\n",
      "========================== Processed 200000 rows ( 8.30159701972667 %)========================== 0.7122833808263143 mins\n",
      "========================== Processed 400000 rows ( 16.60319403945334 %)========================== 1.4862464745839437 mins\n",
      "========================== Processed 600000 rows ( 24.90479105918001 %)========================== 2.3454039971033733 mins\n",
      "========================== Processed 800000 rows ( 33.20638807890668 %)========================== 3.2673895637194317 mins\n",
      "========================== Processed 1000000 rows ( 41.50798509863335 %)========================== 5.184503829479217 mins\n",
      "========================== Processed 1200000 rows ( 49.80958211836002 %)========================== 5.950053683916727 mins\n",
      "========================== Processed 1400000 rows ( 58.11117913808669 %)========================== 6.672865601380666 mins\n",
      "========================== Processed 1600000 rows ( 66.41277615781335 %)========================== 7.474204806486766 mins\n",
      "========================== Processed 1800000 rows ( 74.71437317754003 %)========================== 8.261698611577351 mins\n",
      "========================== Processed 2000000 rows ( 83.0159701972667 %)========================== 9.035804839928945 mins\n",
      "========================== Processed 2200000 rows ( 91.31756721699337 %)========================== 9.835585141181946 mins\n",
      "========================== Processed 2400000 rows ( 99.61916423672004 %)========================== 11.829185767968495 mins\n",
      "[Test set] Average reward for optimal RL policy:  3.5870054323020537\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following RL optimal policy\n",
    "start_time = time.time()\n",
    "\n",
    "avg_rewards = []\n",
    "for i, row in test_sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(test_sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    s = row['state']\n",
    "    mdp.reset(s) # Set start state to `s`\n",
    "\n",
    "    r = get_average_reward(s, mdp, state_values, gamma)\n",
    "    avg_rewards.append(r)\n",
    "\n",
    "print(\"[Test set] Average reward for optimal RL policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Processed 0 rows ( 0.0 %)========================== 0.024102195103963216 mins\n",
      "========================== Processed 200000 rows ( 8.30159701972667 %)========================== 0.8586580236752828 mins\n",
      "========================== Processed 400000 rows ( 16.60319403945334 %)========================== 1.4298640052477518 mins\n",
      "========================== Processed 600000 rows ( 24.90479105918001 %)========================== 2.084144035975138 mins\n",
      "========================== Processed 800000 rows ( 33.20638807890668 %)========================== 2.6806419491767883 mins\n",
      "========================== Processed 1000000 rows ( 41.50798509863335 %)========================== 3.2647037069002787 mins\n",
      "========================== Processed 1200000 rows ( 49.80958211836002 %)========================== 3.9284449934959413 mins\n",
      "========================== Processed 1400000 rows ( 58.11117913808669 %)========================== 4.595783428351084 mins\n",
      "========================== Processed 1600000 rows ( 66.41277615781335 %)========================== 5.249728310108185 mins\n",
      "========================== Processed 1800000 rows ( 74.71437317754003 %)========================== 5.814100130399068 mins\n",
      "========================== Processed 2000000 rows ( 83.0159701972667 %)========================== 6.46324516137441 mins\n",
      "========================== Processed 2200000 rows ( 91.31756721699337 %)========================== 7.096437788009643 mins\n",
      "========================== Processed 2400000 rows ( 99.61916423672004 %)========================== 7.769247023264567 mins\n",
      "[Test set] Average reward for random policy:  0.9416974307313686\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward following random policy\n",
    "start_time = time.time()\n",
    "\n",
    "avg_rewards = []\n",
    "\n",
    "for i, row in test_sample_episodes.iterrows():\n",
    "    if row['bloc'] != 0:\n",
    "        continue\n",
    "    \n",
    "    if i % 200000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(test_sample_episodes) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    \n",
    "    s = row['state']\n",
    "    mdp.reset(state=s) # Set start state to `s`\n",
    "\n",
    "    rewards = []\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        random_action = random.choice(mdp.get_possible_actions(s))\n",
    "        \n",
    "        s, r, done, _ = mdp.step(random_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    avg_rewards.append(np.mean(rewards))\n",
    "\n",
    "print(\"[Test set] Average reward for random policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mdp_data.to_csv(test_data_dir + 'mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "test_mdp_episodes.to_csv(test_data_dir + 'mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_states = 6\n",
    "# num_actions = 4\n",
    "\n",
    "# test_mdp_data = pd.read_csv(test_data_dir + 'mdp_data_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)\n",
    "# test_mdp_episodes = pd.read_csv(test_data_dir + 'mdp_episodes_nS=' + str(num_states) + '_nA=' + str(num_actions) + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
