{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_PERIOD: 1\n",
      "nS: 4\n",
      "nA: 7\n"
     ]
    }
   ],
   "source": [
    "TIME_PERIOD = 1\n",
    "nS = 4\n",
    "nA = 7\n",
    "\n",
    "data_dir = './jmp/train/'\n",
    "rl_data_dir = './rl_data/train/' + str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data = pd.read_csv(data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "mdp_transitions = pickle.load(open(rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "mdp_rewards = pickle.load(open(rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# state_values[state]\n",
    "state_values = pickle.load(open(rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Tree conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
    "\n",
    "\"\"\"\n",
    "Implements the following:\n",
    "- MDP class\n",
    "- plot_graph \n",
    "- plot_graph_with_state_values\n",
    "- plot_graph_optimal_strategy_and_state_values\n",
    "\"\"\"\n",
    "\n",
    "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "    \n",
    "    def get_next_state_transitions(self, state, action):\n",
    "        \"\"\" return a list of [ P(next_state1 | state, action), P(next_state1 | state, action), ...] \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        \n",
    "        transitions = self._transition_probs[state][action]\n",
    "        \n",
    "        transition_probabilities = []\n",
    "        for state in sorted(transitions.keys()):\n",
    "            transition_probabilities.append(transitions[state])\n",
    "        \n",
    "        return transition_probabilities\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self, state=None):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if state:\n",
    "            self._current_state = state\n",
    "            return self._current_state\n",
    "        \n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        state_node = str(state_node)\n",
    "        \n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            posible_action = str(posible_action)\n",
    "            \n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    try:\n",
    "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
    "                    action in next_actions]\n",
    "        optimal_action = next_actions[np.argmax(q_values)]\n",
    "    except NameError:\n",
    "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
    "        \n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp,\n",
    "                                                     state_values,\n",
    "                                                     state_node,\n",
    "                                                     gamma):\n",
    "                graph.edge(str(state_node), str(state_node) + \"-\" + str(action),\n",
    "                           **opt_s_a_edge_attrs)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kinda buggy. Needs to be fixed\n",
    "# def get_current_optimal_action(mdp, pre_action_state_probs, post_utilities, debug=False):\n",
    "#     \"\"\"\n",
    "#     Returns optimal action by only considering current time step.\n",
    "#     \"\"\"\n",
    "#     if mdp.is_terminal(state): \n",
    "#         return None\n",
    "        \n",
    "#     optimal_action = None\n",
    "#     optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "#     actions = mdp.get_possible_actions(state)\n",
    "#     pre_action_states = np.arange(nS)\n",
    "\n",
    "#     action_utilities = [0] * nS\n",
    "#     pre_utilities = [0] * nS\n",
    "    \n",
    "#     for i, action in enumerate(actions):\n",
    "#         for j, state in enumerate(pre_action_states):\n",
    "#             post_action_state_probs = mdp.get_next_state_transitions(state, action)\n",
    "            \n",
    "#             pu = get_weighted_sum(post_utilities, post_action_state_probs)\n",
    "#             pre_utilities[j] = pu\n",
    "            \n",
    "#         au = get_weighted_sum(pre_utilities, pre_action_state_probs)\n",
    "#         action_utilities[i] = au\n",
    "    \n",
    "#         if au >= optimal_action_value:\n",
    "#             optimal_action_value = action_value\n",
    "#             optimal_action = action\n",
    "            \n",
    "#         if debug:\n",
    "#             print('action', action, ':', action_value)\n",
    "\n",
    "#     if debug:\n",
    "#         print()\n",
    "\n",
    "#     return optimal_action, optimal_action_value, # root\n",
    "#             actions, action_utilities, # action layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreeRoot = {\n",
    "#     'action_layer': [\n",
    "#         ActionNode_1,\n",
    "#         ActionNode_2,\n",
    "#         ActionNode_4,\n",
    "#     ],\n",
    "#     'time_step': 0,\n",
    "#     'optimal_action': ActionNode_1,\n",
    "# }\n",
    "\n",
    "# ActionNode = {\n",
    "#     'action_name': 'action_1',\n",
    "#     'pre_action_layer': [\n",
    "#         (PreActionStateNode_1, pre_prob_1),\n",
    "#         (PreActionStateNode_2, pre_prob_2),\n",
    "#         (PreActionStateNode_3, pre_prob_3),\n",
    "#         (PreActionStateNode_4, pre_prob_4),\n",
    "#         (PreActionStateNode_5, pre_prob_5),\n",
    "#     ],\n",
    "#     'expected_action_utility': 1.23,\n",
    "# }\n",
    "\n",
    "# PreActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'post_action_layer': [\n",
    "#         (PostActionStateNode_1, post_prob_1),\n",
    "#         (PostActionStateNode_2, post_prob_2),\n",
    "#         (PostActionStateNode_3, post_prob_3),\n",
    "#         (PostActionStateNode_4, post_prob_4),\n",
    "#         (PostActionStateNode_5, post_prob_5),\n",
    "#     ],\n",
    "#     'expected_state_utility': 0.45,\n",
    "# }\n",
    "\n",
    "# PostActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'state_utility': 0.45,\n",
    "# }\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Recursive Tree depth  = look_ahead+1\n",
    "\n",
    "# class Tree {\n",
    "#     optimal_action\n",
    "#     action_layer # ActionNode[]\n",
    "# }\n",
    "\n",
    "# class ActionNode {\n",
    "#     name\n",
    "#     utility\n",
    "#     pre_layer # PreStateNode[]\n",
    "#     next_tree # if `soft` probability. Tree for next time step.\n",
    "# }\n",
    "\n",
    "# class PreStateNode {\n",
    "#     name\n",
    "#     probability # uncertanity\n",
    "#     utility # expected reward\n",
    "#     post_layer # PostStateNode[]\n",
    "#     next_tree # if `hard` probability and this is most probable current state. Tree for next time step.\n",
    "# }\n",
    "\n",
    "# class PostStateNode {\n",
    "#     name\n",
    "#     probability # transition\n",
    "#     utility # reward obtained\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After action is chosen, for next time step:\n",
    "\n",
    "# 1. Soft Prob\n",
    "\n",
    "# - pre_action_state_probs = pre_action_state_probs * Transitions.T\n",
    "#       For each s', p'(s') = Sum [ p(s) * T(s, a, s') ] for all s\n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = post_utilities (we get reward at end state as o/p) / pre_utilities (we get cumulative rewards as o/p) ?\n",
    "#         o/p refers to action utility for this time step\n",
    "\n",
    "\n",
    "# 2. Hard Prob (assume that we were in the most probable state- and the corresponding values were our action utilities)\n",
    "\n",
    "# Cannot have more than 1 additional time step. Becuase we don't know for sure what state we are in for timestep t+1\n",
    "\n",
    "# - pre_action_state_probs = post_action_state_probs \n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = pre_utilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For computing optimal action with lookahead, we use soft prob and choose the root action that leads \n",
    "# to the maximax/maximin of the action utilities at the final lookahead timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_PERIOD == 1:\n",
    "    file_name = 'all_data-1_month_period-0.15_subset-' + str(nS) + '_cluster_means.csv'\n",
    "elif TIME_PERIOD == 2:\n",
    "    file_name = 'all_data-2_month_period-0.2_subset-' + str(nS) + '_cluster_means.csv'\n",
    "else:\n",
    "    raise Error('no cluster means file for TIME_PERIOD=', TIME_PERIOD)\n",
    "\n",
    "cluster_means = pd.read_csv(data_dir + file_name)\n",
    "cluster_means = np.array(cluster_means)\n",
    "\n",
    "\n",
    "def distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Returns Euclidean distance between 2 points.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2)) \n",
    "\n",
    "def get_mdp_state_probabilities(row):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - row: dict/pd.Series containing the values of the dimensionality reduction features (\"Cluster * Components\")\n",
    "    \"\"\"\n",
    "    v = []\n",
    "    for i in range(1, 5 + 1):\n",
    "        v.append(row['Cluster ' + str(i) + ' Components'])\n",
    "    v = np.array(v)\n",
    "    \n",
    "    distances = []\n",
    "    for cluster in range(nS):\n",
    "        distances.append(distance(v, cluster_means[cluster]))\n",
    "    \n",
    "    probs = 1 - (distances / np.sum(np.abs(distances),axis=0))\n",
    "\n",
    "    return probs, np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pre_action_state_probs(prob_mode, pre_probs, mdp, action):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - mdp: MDP with the transition matrix. To be used for `soft` mode.\n",
    "    \"\"\"\n",
    "    if prob_mode == 'soft':        \n",
    "        transition_mat = [[]] * nS # from curr_state to next_state, if action is taken.\n",
    "        for state in range(nS):\n",
    "            if action not in mdp.get_possible_actions(state):\n",
    "                transition_mat[state] = [0] * nS\n",
    "                continue\n",
    "            transition_mat[state] = mdp.get_next_state_transitions(state, action)\n",
    "\n",
    "        next_pre_probs = [0] * nS\n",
    "        for next_s in range(nS):\n",
    "            for curr_s in range(nS):\n",
    "                next_pre_probs[next_s] += pre_probs[curr_s] * transition_mat[curr_s][next_s]\n",
    "        \n",
    "        # FIXME: if state cannot perform action, then we have a loss of probability mass\n",
    "#         assert math.isclose(sum(next_pre_probs), 1, rel_tol=1e-6), 'Sum=' + str(sum(next_pre_probs)) + ' is not 1'\n",
    "        \n",
    "    elif prob_mode == 'hard':\n",
    "        # Choose the transition probabilities by assuming that we are in a hard state- the most probable state.\n",
    "        most_probable_state = np.argmax(pre_probs)\n",
    "        while action not in mdp.get_possible_actions(most_probable_state):\n",
    "            pre_probs[most_probable_state] = 0\n",
    "            most_probable_state = np.argmax(pre_probs)\n",
    "        \n",
    "        next_pre_probs = mdp.get_next_state_transitions(most_probable_state, action)\n",
    "    \n",
    "    else:\n",
    "        raise Error(\"invalid prob_mode=\", prob_mode)\n",
    "    \n",
    "    return next_pre_probs\n",
    "\n",
    "def update_post_action_state_utilities(utility_mode, pre_utilities, post_utilities):\n",
    "    if utility_mode == 'static':\n",
    "        next_post_utilities = post_utilities\n",
    "    \n",
    "    elif utility_mode == 'dynamic':\n",
    "        next_post_utilities = pre_utilities # expected utility at pre action state layer\n",
    "    \n",
    "    else:\n",
    "        raise Error(\"invalid utility_mode=\", utility_mode)\n",
    "    \n",
    "    return next_post_utilities\n",
    "\n",
    "def get_weighted_sum(vals, weights):\n",
    "    v = np.array(vals)\n",
    "    w = np.array(weights)\n",
    "    \n",
    "    return v.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor to objects\n",
    "\n",
    "def get_future_actions_utility(future_action_utilities, mt_mode):\n",
    "    \"\"\"\n",
    "    Returns utility that is maximum/minimum of the action utilities.\n",
    "    \n",
    "    If Markov Tree goal is to maximise the maximum overall reward ('maximax'), \n",
    "    returns maximum of the future action utilies.\n",
    "    \n",
    "    If goal is to maximise the minimum overall reward ('maximin'), returns minimum\n",
    "    of the future action utilities.\n",
    "    \n",
    "    Params:\n",
    "        - mt_mode: Goal of the Markov Tree: 'maximax' or 'maximin'\n",
    "    \"\"\"\n",
    "    if mt_mode == 'maximax':\n",
    "        return max(future_action_utilities)\n",
    "    elif mt_mode == 'maximin':\n",
    "        return min(future_action_utilities)\n",
    "    else:\n",
    "        # TODO: add maxiavg ?\n",
    "        raise Error('invalid Markov Tree mode. mt_mode=' + mt_mode)\n",
    "\n",
    "\n",
    "def get_action_lookahead_utilities(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns long term reward (up to look_ahead time steps) for taking this action\n",
    "\n",
    "    Params:\n",
    "        - mdp:\n",
    "        - pre_probs: \n",
    "        - post_utilities:\n",
    "        - mt_mode:\n",
    "        - prob_mode:\n",
    "        - utility_mode:\n",
    "        - look_ahead: \n",
    "        - gamma: \n",
    "        - debug:    \n",
    "    \n",
    "    Returns:\n",
    "        - action_utilities:\n",
    "    \"\"\"\n",
    "    # action_utilities: long term reward (till look_ahead time steps) for taking each action\n",
    "    action_utilities = [0] * nA   \n",
    "\n",
    "    if kwargs['look_ahead'] < 0:\n",
    "        return action_utilities\n",
    "    \n",
    "    for action in range(nA):\n",
    "        # Compute all values for the Markov Tree at this time step\n",
    "        pre_utilities = [0] * nS\n",
    "        \n",
    "        for state in range(nS):\n",
    "            if action not in kwargs['mdp'].get_possible_actions(state):\n",
    "                continue\n",
    "            \n",
    "            post_probs = kwargs['mdp'].get_next_state_transitions(state, action)\n",
    "            pre_utilities[state] = get_weighted_sum(kwargs['post_utilities'], post_probs)\n",
    "        \n",
    "        action_utilities[action] = get_weighted_sum(pre_utilities, kwargs['pre_probs'])\n",
    "    \n",
    "        # Now, we move to the next time step assuming that we picked this `action` at this time step.\n",
    "        # Update probabilities and utilities inputs for next time step.\n",
    "        next_pre_probs = update_pre_action_state_probs(kwargs['prob_mode'], kwargs['pre_probs'], kwargs['mdp'], action)\n",
    "        next_post_utilities = update_post_action_state_utilities(kwargs['utility_mode'], pre_utilities, kwargs['post_utilities'])\n",
    "\n",
    "        future_action_utilities = get_action_lookahead_utilities(pre_probs=next_pre_probs,\n",
    "                                                     post_utilities=next_post_utilities,\n",
    "                                                     look_ahead=kwargs['look_ahead']-1,\n",
    "                                                     gamma=kwargs['gamma'], # FIXME: kwargs['gamma']**2 ?? currently, the factor gets multiplied below?\n",
    "                                                     mdp=kwargs['mdp'],\n",
    "                                                     mt_mode=kwargs['mt_mode'],\n",
    "                                                     prob_mode=kwargs['prob_mode'],\n",
    "                                                     utility_mode=kwargs['utility_mode'],\n",
    "                                                     debug=kwargs['debug']\n",
    "                                                    )\n",
    "        action_utilities[action] += kwargs['gamma'] * get_future_actions_utility(future_action_utilities, kwargs['mt_mode'])\n",
    "        \n",
    "    return action_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_markov_tree_action(mdp, pre_probs, post_utilities, \n",
    "                                   mt_mode, prob_mode, utility_mode, \n",
    "                                   look_ahead=0, gamma=1, debug=False):\n",
    "    \"\"\"\n",
    "    Performs a RL-like lookahead mechanism to find the optimal action for the current Markov Tree.\n",
    "    It is like RL, but explainable through the decision tree structure.\n",
    "    \n",
    "    Params:\n",
    "    - mdp: MDP\n",
    "    - pre_probs: list of probabilities of being in each state in pre action state layer\n",
    "    - post_utilities: state values at each state in the post action state layer (leaf nodes)\n",
    "    - mt_mode: 'maximax' | 'maximin'\n",
    "    - prob_mode: 'soft' | 'hard'\n",
    "    - utility_mode: 'static' | 'dynamic'\n",
    "    - look_ahead: how many future time steps should be considered before deciding on the \n",
    "                    optimal action. 0 means no additional time steps.\n",
    "    - gamma: the discount factor for how much the weight factor for future rewards decreases.\n",
    "    \n",
    "    Tree at timestep t:\n",
    "    decision node (root) -> action layer -> pre action state layer -> post action state layer (leaves)\n",
    "    \"\"\"\n",
    "    assert mt_mode in ['maximax', 'maximin']\n",
    "    assert prob_mode in ['hard', 'soft']\n",
    "    assert utility_mode in ['static', 'dynamic']\n",
    "    \n",
    "    action_utilities = get_action_lookahead_utilities(pre_probs=pre_probs,\n",
    "                                          post_utilities=post_utilities,\n",
    "                                          look_ahead=look_ahead,\n",
    "                                          gamma=gamma,\n",
    "                                          mdp=mdp,\n",
    "                                          mt_mode=mt_mode,\n",
    "                                          prob_mode=prob_mode,\n",
    "                                          utility_mode=utility_mode,\n",
    "                                          debug=debug\n",
    "                                        )\n",
    "    \n",
    "    optimal_action = np.argmax(action_utilities)\n",
    "    \n",
    "    return optimal_action, action_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run train data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = np.arange(nS)\n",
    "mdp = MDP(mdp_transitions, mdp_rewards, initial_state=random.choice(initial_states), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(mdp, state_values, start_state, rows, debug=False):\n",
    "    \"\"\"\n",
    "    Returns the average reward for the episode of length EPISODE_LENGTH \n",
    "    starting at `start_state`, when following the MARKOV TREE policy. \n",
    "    \n",
    "    Params:\n",
    "        - rows: data points for a single donor\n",
    "    \"\"\"    \n",
    "    if debug:\n",
    "        print('=============================================================================================\\n')\n",
    "        \n",
    "    mdp.reset(start_state)\n",
    "\n",
    "    s = start_state\n",
    "    rewards = []\n",
    "    for i, row in rows.reset_index(drop=True).iterrows():\n",
    "        if debug:\n",
    "            print('********* Time step:', i, '| Current state:', s, '*********')\n",
    "            \n",
    "        # TODO: How to evaluate Markov Tree prospectively?\n",
    "        # FIXME: should be current state, not `start_row`\n",
    "        # Need to use EPISODE_LENGTH ?\n",
    "        \n",
    "        pre_action_state_probs, closest_state = get_mdp_state_probabilities(row)\n",
    "        post_action_state_utilities = [state_values[s] for s in range(nS)]\n",
    "                \n",
    "        optimal_action, action_utilities = get_optimal_markov_tree_action(mdp, pre_action_state_probs, post_action_state_utilities, \n",
    "                                       MT_MODE, PROB_MODE, UTILITY_MODE, \n",
    "                                       LOOK_AHEAD, GAMMA, debug=debug)\n",
    "        \n",
    "        if debug:\n",
    "            print('Optimal action', optimal_action, '   |    Final action utilities:', action_utilities)\n",
    "            \n",
    "        while optimal_action not in mdp.get_possible_actions(s):\n",
    "            action_utilities[optimal_action] = 0\n",
    "            optimal_action = np.argmax(action_utilities)\n",
    "        \n",
    "        if debug:\n",
    "            print('  >>> Performing action:', optimal_action)\n",
    "        ac.append(optimal_action)\n",
    "        s, r, done, _ = mdp.step(optimal_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length_months = 50\n",
    "EPISODE_LENGTH = episode_length_months // TIME_PERIOD # total num months / TIME_PERIOD of each event\n",
    "\n",
    "PROB_MODE = 'hard' # 'hard' 'soft'\n",
    "UTILITY_MODE = 'dynamic' # 'static'  'dynamic'\n",
    "\n",
    "# zero because we already use RL state values at the leaves. These give an indication of the cumulative future rewards.\n",
    "look_ahead_months = 0 \n",
    "LOOK_AHEAD = look_ahead_months // TIME_PERIOD # num months lookahead / TIME_PERIOD\n",
    "\n",
    "MT_MODE = 'maximin' # 'maximax'  'maximin'\n",
    "\n",
    "# Discount factor γ / gamma\n",
    "# If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward. \n",
    "# If γ=1, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
    "GAMMA = 0 #0.95\n",
    "\n",
    "debug = True\n",
    "\n",
    "donor_ids_sample = np.random.choice(donor_data['id'].unique(), 100)\n",
    "# donor_sample_data = donor_data[donor_data['bloc'] == 0] # initial data rows\n",
    "# donor_sample_data = donor_data[donor_data['id'].isin(donor_ids_sample)]\n",
    "\n",
    "ac = []\n",
    "start_time = time.time()\n",
    "avg_rewards = []\n",
    "for i, donor_id in enumerate(donor_ids_sample):\n",
    "    if i % 100 == 0:\n",
    "        print('========== Processed', i, 'rows (', i / len(donor_ids_sample) * 100,'%) |', \n",
    "              'Avg reward =',  np.mean(avg_rewards), '==========', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    rows = donor_data[donor_data['id'] == donor_id][0:1]\n",
    "    \n",
    "    r = get_average_reward(mdp, state_values, rows.iloc[0]['State_Cluster_' + str(nS)], rows, debug)\n",
    "    print(\"\\n\\nDonor average reward. Markov Tree policy:\", r, \" |  Actual:\", sum(rows['reward']), '/', len(rows), '=', sum(rows['reward']) / len(rows))    \n",
    "\n",
    "    avg_rewards.append(r)    \n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('Exited at i =', i, 'to prevent excessive logs and processing time')\n",
    "        break\n",
    "\n",
    "print(\"Average reward for Markov Tree policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_PERIOD: 1\n",
      "nS: 4\n",
      "nA: 7\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = './jmp/test/'\n",
    "test_rl_data_dir = './rl_data/test/'+ str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69.26948037604721, 68.41701202532855, 62.49339304026267, 41.6480539541481]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([69.26948037604721, 68.41701202532855, 62.49339304026267, 41.6480539541481])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([state_values[s] for s in range(nS)])\n",
    "state_values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './rl_data/test/1_month_period/mdp_transitions_dict_nS=4_nA=7.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e4e400d9b050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# test_mdp_transitions[curr_state][action][next_state] - some have missing actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_mdp_transitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rl_data_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'mdp_transitions_dict_nS='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_nA='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# test_mdp_rewards[curr_state][action][next_state] - some have missing actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_mdp_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_rl_data_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'mdp_rewards_dict_nS='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_nA='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './rl_data/test/1_month_period/mdp_transitions_dict_nS=4_nA=7.p'"
     ]
    }
   ],
   "source": [
    "test_donor_data = pd.read_csv(test_data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')\n",
    "\n",
    "# test_mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_transitions = pickle.load(open(test_rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_rewards = pickle.load(open(test_rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_state_values[state]\n",
    "test_state_values = pickle.load(open(test_rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
