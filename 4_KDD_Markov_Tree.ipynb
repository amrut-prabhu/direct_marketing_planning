{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_PERIOD: 1\n",
      "nS: 13\n",
      "nA: 7\n"
     ]
    }
   ],
   "source": [
    "TIME_PERIOD = 1\n",
    "nS = 13\n",
    "nA = 7\n",
    "\n",
    "data_dir = './jmp/train/'\n",
    "rl_data_dir = './rl_data/train/' + str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data = pd.read_csv(data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "mdp_transitions = pickle.load(open(rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "mdp_rewards = pickle.load(open(rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# state_values[state]\n",
    "state_values = pickle.load(open(rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Tree conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
    "\n",
    "\"\"\"\n",
    "Implements the following:\n",
    "- MDP class\n",
    "- plot_graph \n",
    "- plot_graph_with_state_values\n",
    "- plot_graph_optimal_strategy_and_state_values\n",
    "\"\"\"\n",
    "\n",
    "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self, state=None):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if state:\n",
    "            self._current_state = state\n",
    "            return self._current_state\n",
    "        \n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        state_node = str(state_node)\n",
    "        \n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            posible_action = str(posible_action)\n",
    "            \n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    try:\n",
    "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
    "                    action in next_actions]\n",
    "        optimal_action = next_actions[np.argmax(q_values)]\n",
    "    except NameError:\n",
    "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
    "        \n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp,\n",
    "                                                     state_values,\n",
    "                                                     state_node,\n",
    "                                                     gamma):\n",
    "                graph.edge(str(state_node), str(state_node) + \"-\" + str(action),\n",
    "                           **opt_s_a_edge_attrs)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_current_optimal_action(mdp, pre_action_state_probs, post_utilities, debug=False):\n",
    "#     if mdp.is_terminal(state): \n",
    "#         return None\n",
    "        \n",
    "#     optimal_action = None\n",
    "#     optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "#     actions = mdp.get_possible_actions(state)\n",
    "#     pre_action_states = np.arange(nS)\n",
    "\n",
    "#     action_utilities = [] * nS\n",
    "#     pre_utilities = [] * nS\n",
    "    \n",
    "#     for i, action in enumerate(actions):\n",
    "#         for j, state in enumerate(pre_action_states):\n",
    "#             post_action_state_probs = mdp.get_next_states(state, action).values()\n",
    "            \n",
    "#             pu = get_weighted_sum(post_utilities, post_action_state_probs)\n",
    "#             pre_utilities[j] = pu\n",
    "            \n",
    "#         au = get_weighted_sum(pre_utilities, pre_action_state_probs)\n",
    "#         action_utilities[i] = au\n",
    "    \n",
    "#         if au >= optimal_action_value:\n",
    "#             optimal_action_value = action_value\n",
    "#             optimal_action = action\n",
    "            \n",
    "#         if debug:\n",
    "#             print('action', action, ':', action_value)\n",
    "\n",
    "#     if debug:\n",
    "#         print()\n",
    "\n",
    "#     return optimal_action, optimal_action_value, # root\n",
    "#             actions, action_utilities, # action layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreeRoot = {\n",
    "#     'action_layer': [\n",
    "#         ActionNode_1,\n",
    "#         ActionNode_2,\n",
    "#         ActionNode_4,\n",
    "#     ],\n",
    "#     'time_step': 0,\n",
    "#     'optimal_action': ActionNode_1,\n",
    "# }\n",
    "\n",
    "# ActionNode = {\n",
    "#     'action_name': 'action_1',\n",
    "#     'pre_action_layer': [\n",
    "#         (PreActionStateNode_1, pre_prob_1),\n",
    "#         (PreActionStateNode_2, pre_prob_2),\n",
    "#         (PreActionStateNode_3, pre_prob_3),\n",
    "#         (PreActionStateNode_4, pre_prob_4),\n",
    "#         (PreActionStateNode_5, pre_prob_5),\n",
    "#     ],\n",
    "#     'expected_action_utility': 1.23,\n",
    "# }\n",
    "\n",
    "# PreActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'post_action_layer': [\n",
    "#         (PostActionStateNode_1, post_prob_1),\n",
    "#         (PostActionStateNode_2, post_prob_2),\n",
    "#         (PostActionStateNode_3, post_prob_3),\n",
    "#         (PostActionStateNode_4, post_prob_4),\n",
    "#         (PostActionStateNode_5, post_prob_5),\n",
    "#     ],\n",
    "#     'expected_state_utility': 0.45,\n",
    "# }\n",
    "\n",
    "# PostActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'state_utility': 0.45,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After action is chosen, for next time step:\n",
    "\n",
    "# 1. Soft Prob\n",
    "\n",
    "# - pre_action_state_probs = pre_action_state_probs * Transitions.T\n",
    "#       For each s', p'(s') = Sum [ p(s) * T(s, a, s') ] for all s\n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = post_utilities (we get reward at end state as o/p) / pre_utilities (we get cumulative rewards as o/p) ?\n",
    "#         o/p refers to action utility for this time step\n",
    "\n",
    "\n",
    "# 2. Hard Prob (assume that we were in the most probable state- and the corresponding values were our action utilities)\n",
    "\n",
    "# Cannot have more than 1 additional time step. Becuase we don't know for sure what state we are in for timestep t+1\n",
    "\n",
    "# - pre_action_state_probs = post_action_state_probs \n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = pre_utilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For computing optimal action with lookahead, we use soft prob and choose the root action that leads \n",
    "# to the maximax/maximin of the action utilities at the final lookahead timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_PERIOD == 1:\n",
    "    file_name = 'all_data-1_month_period-0.15_subset-' + str(nS) + '_cluster_means.csv'\n",
    "elif TIME_PERIOD == 2:\n",
    "    file_name = 'all_data-2_month_period-0.2_subset-' + str(nS) + '_cluster_means.csv'\n",
    "else:\n",
    "    raise Error('no cluster means file for TIME_PERIOD=', TIME_PERIOD)\n",
    "\n",
    "cluster_means = pd.read_csv(data_dir + file_name)\n",
    "cluster_means = np.array(cluster_means)\n",
    "\n",
    "\n",
    "def distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Returns Euclidean distance between 2 points.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2)) \n",
    "\n",
    "def get_mdp_state_probabilities(row):\n",
    "    v = []\n",
    "    for i in range(1, 5 + 1):\n",
    "        v.append(row['Cluster ' + str(i) + ' Components'])\n",
    "    v = np.array(v)\n",
    "    \n",
    "    distances = []\n",
    "    for cluster in range(nS):\n",
    "        distances.append(distance(v, cluster_means[cluster]))\n",
    "    \n",
    "    probs = distances / np.sum(np.abs(distances),axis=0)\n",
    "\n",
    "    return probs, np.argmin(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor to objects\n",
    "\n",
    "def get_weighted_sum(vals, weights):\n",
    "    v = np.array(vals)\n",
    "    w = np.array(weights)\n",
    "    \n",
    "    return v.dot(w)\n",
    "\n",
    "def get_action_results(mdp, pre_action_state_probs, post_utilities, debug=False):\n",
    "    if mdp.is_terminal(state): \n",
    "        return None\n",
    "        \n",
    "    optimal_action = None\n",
    "    optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    pre_action_states = np.arange(nS)\n",
    "\n",
    "    action_utilities = [] * nS\n",
    "    pre_utilities = [] * nS\n",
    "    \n",
    "    for i, action in enumerate(actions):\n",
    "        for j, state in enumerate(pre_action_states):\n",
    "            post_action_state_probs = mdp.get_next_states(state, action).values()\n",
    "            \n",
    "            pu = get_weighted_sum(post_utilities, post_action_state_probs)\n",
    "            pre_utilities[j] = pu\n",
    "            \n",
    "        au = get_weighted_sum(pre_utilities, pre_action_state_probs)\n",
    "        action_utilities[i] = au\n",
    "    \n",
    "        # TODO: Need to decide what final target metric is- maximax or maximin\n",
    "        \n",
    "        if au >= optimal_action_value:\n",
    "            optimal_action_value = action_value\n",
    "            optimal_action = action\n",
    "            \n",
    "        if debug:\n",
    "            print('action', action, ':', action_value)\n",
    "\n",
    "    if debug:\n",
    "        print()\n",
    "\n",
    "    return optimal_action, optimal_action_value, # root\n",
    "    actions, action_utilities, # action layer\n",
    "    pre_action_state_probs, pre_utilities, # pre action state layer\n",
    "    post_action_state_probs, post_utilities # post action state layer\n",
    "\n",
    "def update_pre_action_state_probabilities(prob_mode, pre_state_probs, post_state_probs, mdp, action):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - mdp: MDP with the transition matrix. To be used for `soft` mode.\n",
    "\n",
    "        - post_state_probs: transition probs, starting from a specific pre action state. To be used for `hard` mode.\n",
    "    \"\"\"\n",
    "    if prob_mode == 'soft':        \n",
    "        transition_mat = [[]] * nS\n",
    "        for state in range(nS):\n",
    "            transition_mat[state] = [0] * nS\n",
    "            \n",
    "            transitions = mdp.get_next_states(state, action)\n",
    "            for next_s in range(nS):\n",
    "                transition_mat[state][next_s] = transitions[next_s]\n",
    "\n",
    "\n",
    "        next_pre_action_state_probs = [0] * nS\n",
    "        for dest in range(nS):\n",
    "            for src in range(nS):\n",
    "                next_pre_action_state_probs[dest] += pre_state_probs[src] * transition_mat[src][dest]\n",
    "        \n",
    "        print(sum(next_pre_action_state_probs))\n",
    "\n",
    "    elif prob_mode == 'hard':\n",
    "        next_pre_action_state_probs = post_state_probs\n",
    "    else:\n",
    "        raise Error(\"invalid prob_mode=\", prob_mode)\n",
    "    \n",
    "    return next_pre_action_state_probs\n",
    "\n",
    "def update_post_action_state_utilities(utility_mode, pre_utilities, post_utilities):\n",
    "    if utility_mode == 'static':\n",
    "        next_post_utilities = post_utilities\n",
    "    elif utility_mode == 'dynamic':\n",
    "        next_post_utilities = pre_utilities # expected utility at pre action state layer\n",
    "    else:\n",
    "        raise Error(\"invalid utility_mode=\", utility_mode)\n",
    "    \n",
    "    return next_post_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_markov_tree_action(mdp, pre_action_state_probs, post_utilities, \n",
    "                                   prob_mode, utility_mode, look_ahead=0, gamma=1, debug=False):\n",
    "    \"\"\"\n",
    "    Performs a RL-like lookahead mechanism to find the optimal action for the current Markov Tree.\n",
    "    It is like RL, but explainable through the decision tree structure.\n",
    "    \n",
    "    Params:\n",
    "    - mdp: MDP\n",
    "    - pre_action_state_probs: list of probabilities of being in each state in pre action state layer\n",
    "    - post_utilities: state values at each state in the post action state layer (leaf nodes)\n",
    "    - prob_mode: 'soft' | 'hard'\n",
    "    - utility_mode: 'static' | 'dynamic'\n",
    "    - look_ahead: how many future time steps should be considered before deciding on the \n",
    "                    optimal action. 0 means no additional time steps.\n",
    "    \n",
    "    Tree at timestep t:\n",
    "    decision node (root) -> action layer -> pre action state layer -> post action state layer (leaves)\n",
    "    \"\"\"\n",
    "    while look_ahead > 0:\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        for action in actions:\n",
    "            optimal_action, optimal_action_value, \n",
    "            actions, action_utilities, \n",
    "            pre_action_state_probs, pre_utilities, \n",
    "            post_action_state_probs, post_utilities = \\\n",
    "                            get_action_results(mdp, pre_action_state_probs, post_utilities, debug)\n",
    "\n",
    "            # Update probabilities and utilities inputs for next time step\n",
    "            pre_action_state_probs = update_pre_action_state_probabilities(prob_mode, \n",
    "                                                   pre_action_state_probs, post_action_state_probs, action)\n",
    "            post_utilities = update_post_action_state_utilities(utility_mode, pre_utilities, post_utilities)\n",
    "\n",
    "            # TODO: update action value * gamma if `dynamic`?\n",
    "        \n",
    "    return rec_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run train data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = np.arange(nS)\n",
    "mdp = MDP(mdp_transitions, mdp_rewards, initial_state=random.choice(initial_states), seed=1)\n",
    "\n",
    "# Discount factor γ / gamma\n",
    "# If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward. \n",
    "# If γ=1, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_LENGTH = 50 / TIME_PERIOD # total num months / TIME_PERIOD of each event\n",
    "\n",
    "PROB_MODE =  # 'hard' 'soft'\n",
    "UTILITY_MODE =  # 'static'  'dynamic'\n",
    "LOOK_AHEAD =  / TIME_PERIOD # num months lookahead / TIME_PERIOD\n",
    "\n",
    "def get_average_reward(start_state, mdp, state_values, gamma, start_row):\n",
    "    \"\"\"\n",
    "    Returns the average reward for the episode of length EPISODE_LENGTH \n",
    "    starting at `start_state`, when following the MARKOV TREE policy. \n",
    "    \"\"\"    \n",
    "    mdp.reset(start_state)\n",
    "\n",
    "    s = start_state\n",
    "    rewards = []\n",
    "    for i in range(EPISODE_LENGTH):\n",
    "        pre_action_state_probs = get_mdp_state_probabilities(start_row)\n",
    "        rec_action = get_optimal_markov_tree_action(mdp, pre_action_state_probs, state_values.values(), \n",
    "                                                    PROB_MODE, UTILITY_MODE, LOOK_AHEAD, GAMMA, debug=False)\n",
    "        \n",
    "        s, r, done, _ = mdp.step(rec_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_donor_data = donor_data[donor_data['bloc'] == 0]\n",
    "\n",
    "start_time = time.time()\n",
    "avg_rewards = []\n",
    "for i, row in initial_donor_data.iterrows():\n",
    "    if i % 10000 == 0:\n",
    "        print('========================== Processed', i, 'rows (', i / len(initial_donor_data) * 100,'%)==========================', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    r = get_average_reward(row['State_Cluster_' + str(nS)], mdp, state_values, gamma, row)\n",
    "    avg_rewards.append(r)    \n",
    "\n",
    "print(\"Average reward for Markov Tree policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Average reward for Markov Tree with lookahead policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './jmp/test/'\n",
    "test_rl_data_dir = './rl_data/test/'+ str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_donor_data = pd.read_csv(test_data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')\n",
    "\n",
    "# test_mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_transitions = pickle.load(open(test_rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_rewards = pickle.load(open(test_rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_state_values[state]\n",
    "test_state_values = pickle.load(open(test_rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
