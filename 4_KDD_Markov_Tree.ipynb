{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_PERIOD: 2\n",
      "nS: 4\n",
      "nA: 7\n"
     ]
    }
   ],
   "source": [
    "TIME_PERIOD = 2\n",
    "nS = 4\n",
    "nA = 7\n",
    "\n",
    "data_dir = './jmp/train/'\n",
    "rl_data_dir = './rl_data/train/' + str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_data = pd.read_csv(data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "mdp_transitions = pickle.load(open(rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "mdp_rewards = pickle.load(open(rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# state_values[state]\n",
    "state_values = pickle.load(open(rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Tree conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Practical Reinforcement Learning on Coursera https://www.coursera.org/learn/practical-rl/\n",
    "\n",
    "\"\"\"\n",
    "Implements the following:\n",
    "- MDP class\n",
    "- plot_graph \n",
    "- plot_graph_with_state_values\n",
    "- plot_graph_optimal_strategy_and_state_values\n",
    "\"\"\"\n",
    "\n",
    "# Most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "    \n",
    "    def get_next_state_transitions(self, state, action):\n",
    "        \"\"\" return a list of [ P(next_state1 | state, action), P(next_state1 | state, action), ...] \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        \n",
    "        transitions = self._transition_probs[state][action]\n",
    "        \n",
    "        transition_probabilities = []\n",
    "        for state in sorted(transitions.keys()):\n",
    "            transition_probabilities.append(transitions[state])\n",
    "        \n",
    "        return transition_probabilities\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self, state=None):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if state:\n",
    "            self._current_state = state\n",
    "            return self._current_state\n",
    "        \n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        state_node = str(state_node)\n",
    "        \n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            posible_action = str(posible_action)\n",
    "            \n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    try:\n",
    "        q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n",
    "                    action in next_actions]\n",
    "        optimal_action = next_actions[np.argmax(q_values)]\n",
    "    except NameError:\n",
    "        raise NameError(\"Implement and run the cell that has the get_action_value function\")\n",
    "        \n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(str(state_node),\n",
    "                   label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp,\n",
    "                                                     state_values,\n",
    "                                                     state_node,\n",
    "                                                     gamma):\n",
    "                graph.edge(str(state_node), str(state_node) + \"-\" + str(action),\n",
    "                           **opt_s_a_edge_attrs)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kinda buggy. Needs to be fixed\n",
    "# def get_current_optimal_action(mdp, pre_action_state_probs, post_utilities, debug=False):\n",
    "#     \"\"\"\n",
    "#     Returns optimal action by only considering current time step.\n",
    "#     \"\"\"\n",
    "#     if mdp.is_terminal(state): \n",
    "#         return None\n",
    "        \n",
    "#     optimal_action = None\n",
    "#     optimal_action_value = - float(\"inf\")\n",
    "    \n",
    "#     actions = mdp.get_possible_actions(state)\n",
    "#     pre_action_states = np.arange(nS)\n",
    "\n",
    "#     action_utilities = [0] * nS\n",
    "#     pre_utilities = [0] * nS\n",
    "    \n",
    "#     for i, action in enumerate(actions):\n",
    "#         for j, state in enumerate(pre_action_states):\n",
    "#             post_action_state_probs = mdp.get_next_state_transitions(state, action)\n",
    "            \n",
    "#             pu = get_weighted_sum(post_utilities, post_action_state_probs)\n",
    "#             pre_utilities[j] = pu\n",
    "            \n",
    "#         au = get_weighted_sum(pre_utilities, pre_action_state_probs)\n",
    "#         action_utilities[i] = au\n",
    "    \n",
    "#         if au >= optimal_action_value:\n",
    "#             optimal_action_value = action_value\n",
    "#             optimal_action = action\n",
    "            \n",
    "#         if debug:\n",
    "#             print('action', action, ':', action_value)\n",
    "\n",
    "#     if debug:\n",
    "#         print()\n",
    "\n",
    "#     return optimal_action, optimal_action_value, # root\n",
    "#             actions, action_utilities, # action layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreeRoot = {\n",
    "#     'action_layer': [\n",
    "#         ActionNode_1,\n",
    "#         ActionNode_2,\n",
    "#         ActionNode_4,\n",
    "#     ],\n",
    "#     'time_step': 0,\n",
    "#     'optimal_action': ActionNode_1,\n",
    "# }\n",
    "\n",
    "# ActionNode = {\n",
    "#     'action_name': 'action_1',\n",
    "#     'pre_action_layer': [\n",
    "#         (PreActionStateNode_1, pre_prob_1),\n",
    "#         (PreActionStateNode_2, pre_prob_2),\n",
    "#         (PreActionStateNode_3, pre_prob_3),\n",
    "#         (PreActionStateNode_4, pre_prob_4),\n",
    "#         (PreActionStateNode_5, pre_prob_5),\n",
    "#     ],\n",
    "#     'expected_action_utility': 1.23,\n",
    "# }\n",
    "\n",
    "# PreActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'post_action_layer': [\n",
    "#         (PostActionStateNode_1, post_prob_1),\n",
    "#         (PostActionStateNode_2, post_prob_2),\n",
    "#         (PostActionStateNode_3, post_prob_3),\n",
    "#         (PostActionStateNode_4, post_prob_4),\n",
    "#         (PostActionStateNode_5, post_prob_5),\n",
    "#     ],\n",
    "#     'expected_state_utility': 0.45,\n",
    "# }\n",
    "\n",
    "# PostActionStateNode = {\n",
    "#     'state_name': 'state_1',\n",
    "#     'state_utility': 0.45,\n",
    "# }\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Recursive Tree depth  = look_ahead+1\n",
    "\n",
    "# class Tree {\n",
    "#     optimal_action\n",
    "#     action_layer # ActionNode[]\n",
    "# }\n",
    "\n",
    "# class ActionNode {\n",
    "#     name\n",
    "#     utility\n",
    "#     pre_layer # PreStateNode[]\n",
    "#     next_tree # if `soft` probability. Tree for next time step.\n",
    "# }\n",
    "\n",
    "# class PreStateNode {\n",
    "#     name\n",
    "#     probability # uncertanity\n",
    "#     utility # expected reward\n",
    "#     post_layer # PostStateNode[]\n",
    "#     next_tree # if `hard` probability and this is most probable current state. Tree for next time step.\n",
    "# }\n",
    "\n",
    "# class PostStateNode {\n",
    "#     name\n",
    "#     probability # transition\n",
    "#     utility # reward obtained\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After action is chosen, for next time step:\n",
    "\n",
    "# 1. Soft Prob\n",
    "\n",
    "# - pre_action_state_probs = pre_action_state_probs * Transitions.T\n",
    "#       For each s', p'(s') = Sum [ p(s) * T(s, a, s') ] for all s\n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = post_utilities (we get reward at end state as o/p) / pre_utilities (we get cumulative rewards as o/p) ?\n",
    "#         o/p refers to action utility for this time step\n",
    "\n",
    "\n",
    "# 2. Hard Prob (assume that we were in the most probable state- and the corresponding values were our action utilities)\n",
    "\n",
    "# Cannot have more than 1 additional time step. Becuase we don't know for sure what state we are in for timestep t+1\n",
    "\n",
    "# - pre_action_state_probs = post_action_state_probs \n",
    "# - pre_utilities = compute from below 2\n",
    "# - post_action_state_probs = from MDP\n",
    "# - post_utilities = pre_utilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For computing optimal action with lookahead, we use soft prob and choose the root action that leads \n",
    "# to the maximax/maximin of the action utilities at the final lookahead timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_PERIOD == 1:\n",
    "    file_name = 'all_data-1_month_period-0.15_subset-' + str(nS) + '_cluster_means.csv'\n",
    "elif TIME_PERIOD == 2:\n",
    "    file_name = 'all_data-2_month_period-0.2_subset-' + str(nS) + '_cluster_means.csv'\n",
    "else:\n",
    "    raise Error('no cluster means file for TIME_PERIOD=', TIME_PERIOD)\n",
    "\n",
    "cluster_means = pd.read_csv(data_dir + file_name)\n",
    "cluster_means = np.array(cluster_means)\n",
    "\n",
    "\n",
    "def distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Returns Euclidean distance between 2 points.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((v1 - v2) ** 2)) \n",
    "\n",
    "def get_mdp_state_probabilities(row):\n",
    "    v = []\n",
    "    for i in range(1, 5 + 1):\n",
    "        v.append(row['Cluster ' + str(i) + ' Components'])\n",
    "    v = np.array(v)\n",
    "    \n",
    "    distances = []\n",
    "    for cluster in range(nS):\n",
    "        distances.append(distance(v, cluster_means[cluster]))\n",
    "    \n",
    "    probs = distances / np.sum(np.abs(distances),axis=0)\n",
    "\n",
    "    return probs, np.argmin(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pre_action_state_probs(prob_mode, pre_probs, mdp, action):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - mdp: MDP with the transition matrix. To be used for `soft` mode.\n",
    "    \"\"\"\n",
    "    if prob_mode == 'soft':        \n",
    "        transition_mat = [[]] * nS # from curr_state to next_state, if action is taken.\n",
    "        for state in range(nS):\n",
    "            if action not in mdp.get_possible_actions(state):\n",
    "                transition_mat[state] = [0] * nS\n",
    "                continue\n",
    "            transition_mat[state] = mdp.get_next_state_transitions(state, action)\n",
    "\n",
    "        next_pre_probs = [0] * nS\n",
    "        for next_s in range(nS):\n",
    "            for curr_s in range(nS):\n",
    "                next_pre_probs[next_s] += pre_probs[curr_s] * transition_mat[curr_s][next_s]\n",
    "        \n",
    "        # FIXME: if state cannot perform action, then we have a loss of probability mass\n",
    "#         assert math.isclose(sum(next_pre_probs), 1, rel_tol=1e-6), 'Sum=' + str(sum(next_pre_probs)) + ' is not 1'\n",
    "        \n",
    "    elif prob_mode == 'hard':\n",
    "        # Choose the transition probabilities by assuming that we are in a hard state- the most probable state.\n",
    "        most_probable_state = np.argmax(pre_probs)\n",
    "        while action not in mdp.get_possible_actions(most_probable_state):\n",
    "            pre_probs[most_probable_state] = 0\n",
    "            most_probable_state = np.argmax(pre_probs)\n",
    "        \n",
    "        next_pre_probs = mdp.get_next_state_transitions(most_probable_state, action)\n",
    "    \n",
    "    else:\n",
    "        raise Error(\"invalid prob_mode=\", prob_mode)\n",
    "    \n",
    "    return next_pre_probs\n",
    "\n",
    "def update_post_action_state_utilities(utility_mode, pre_utilities, post_utilities):\n",
    "    if utility_mode == 'static':\n",
    "        next_post_utilities = post_utilities\n",
    "    \n",
    "    elif utility_mode == 'dynamic':\n",
    "        next_post_utilities = pre_utilities # expected utility at pre action state layer\n",
    "    \n",
    "    else:\n",
    "        raise Error(\"invalid utility_mode=\", utility_mode)\n",
    "    \n",
    "    return next_post_utilities\n",
    "\n",
    "def get_weighted_sum(vals, weights):\n",
    "    v = np.array(vals)\n",
    "    w = np.array(weights)\n",
    "    \n",
    "    return v.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor to objects\n",
    "\n",
    "def get_future_actions_utility(future_action_utilities, mt_mode):\n",
    "    \"\"\"\n",
    "    Returns utility that is maximum/minimum of the action utilities.\n",
    "    \n",
    "    If Markov Tree goal is to maximise the maximum overall reward ('maximax'), \n",
    "    returns maximum of the future action utilies.\n",
    "    \n",
    "    If goal is to maximise the minimum overall reward ('maximin'), returns minimum\n",
    "    of the future action utilities.\n",
    "    \n",
    "    Params:\n",
    "        - mt_mode: Goal of the Markov Tree: 'maximax' or 'maximin'\n",
    "    \"\"\"\n",
    "    if mt_mode == 'maximax':\n",
    "        return max(future_action_utilities)\n",
    "    elif mt_mode == 'maximin':\n",
    "        return min(future_action_utilities)\n",
    "    else:\n",
    "        # TODO: add maxiavg ?\n",
    "        raise Error('invalid Markov Tree mode. mt_mode=' + mt_mode)\n",
    "\n",
    "\n",
    "def get_action_lookahead_utilities(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns long term reward (up to look_ahead time steps) for taking this action\n",
    "\n",
    "    Params:\n",
    "        - mdp:\n",
    "        - pre_probs: \n",
    "        - post_utilities:\n",
    "        - mt_mode:\n",
    "        - prob_mode:\n",
    "        - utility_mode:\n",
    "        - look_ahead: \n",
    "        - gamma: \n",
    "        - debug:    \n",
    "    \n",
    "    Returns:\n",
    "        - action_utilities:\n",
    "    \"\"\"\n",
    "    # action_utilities: long term reward (till look_ahead time steps) for taking each action\n",
    "    action_utilities = [0] * nA   \n",
    "\n",
    "    if kwargs['look_ahead'] <= 0:\n",
    "        assert kwargs['look_ahead'] == 0\n",
    "        return action_utilities\n",
    "    \n",
    "    for action in range(nA):\n",
    "        # Compute all values for the Markov Tree at this time step\n",
    "        pre_utilities = [0] * nS\n",
    "        \n",
    "        for state in range(nS):\n",
    "            if action not in kwargs['mdp'].get_possible_actions(state):\n",
    "                continue\n",
    "            \n",
    "            post_probs = kwargs['mdp'].get_next_state_transitions(state, action)\n",
    "            pre_utilities[state] = get_weighted_sum(kwargs['post_utilities'], post_probs)\n",
    "        \n",
    "        action_utilities[action] = get_weighted_sum(pre_utilities, kwargs['pre_probs'])\n",
    "    \n",
    "        # Now, we move to the next time step assuming that we picked this `action` at this time step.\n",
    "        # Update probabilities and utilities inputs for next time step.\n",
    "        next_pre_probs = update_pre_action_state_probs(kwargs['prob_mode'], kwargs['pre_probs'], kwargs['mdp'], action)\n",
    "        next_post_utilities = update_post_action_state_utilities(kwargs['utility_mode'], pre_utilities, kwargs['post_utilities'])\n",
    "\n",
    "        future_action_utilities = get_action_lookahead_utilities(pre_probs=next_pre_probs,\n",
    "                                                     post_utilities=next_post_utilities,\n",
    "                                                     look_ahead=kwargs['look_ahead']-1,\n",
    "                                                     gamma=kwargs['gamma'], # FIXME: kwargs['gamma']**2 ?? currently, the factor gets multiplied below?\n",
    "                                                     mdp=kwargs['mdp'],\n",
    "                                                     mt_mode=kwargs['mt_mode'],\n",
    "                                                     prob_mode=kwargs['prob_mode'],\n",
    "                                                     utility_mode=kwargs['utility_mode'],\n",
    "                                                     debug=kwargs['debug']\n",
    "                                                    )\n",
    "        action_utilities[action] += kwargs['gamma'] * get_future_actions_utility(future_action_utilities, kwargs['mt_mode'])\n",
    "        \n",
    "    return action_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_markov_tree_action(mdp, pre_probs, post_utilities, \n",
    "                                   mt_mode, prob_mode, utility_mode, \n",
    "                                   look_ahead=0, gamma=1, debug=False):\n",
    "    \"\"\"\n",
    "    Performs a RL-like lookahead mechanism to find the optimal action for the current Markov Tree.\n",
    "    It is like RL, but explainable through the decision tree structure.\n",
    "    \n",
    "    Params:\n",
    "    - mdp: MDP\n",
    "    - pre_probs: list of probabilities of being in each state in pre action state layer\n",
    "    - post_utilities: state values at each state in the post action state layer (leaf nodes)\n",
    "    - mt_mode: 'maximax' | 'maximin'\n",
    "    - prob_mode: 'soft' | 'hard'\n",
    "    - utility_mode: 'static' | 'dynamic'\n",
    "    - look_ahead: how many future time steps should be considered before deciding on the \n",
    "                    optimal action. 0 means no additional time steps.\n",
    "    - gamma: the discount factor for how much the weight factor for future rewards decreases.\n",
    "    \n",
    "    Tree at timestep t:\n",
    "    decision node (root) -> action layer -> pre action state layer -> post action state layer (leaves)\n",
    "    \"\"\"\n",
    "    assert mt_mode in ['maximax', 'maximin']\n",
    "    assert prob_mode in ['hard', 'soft']\n",
    "    assert utility_mode in ['static', 'dynamic']\n",
    "    \n",
    "    action_utilities = get_action_lookahead_utilities(pre_probs=pre_probs,\n",
    "                                          post_utilities=post_utilities,\n",
    "                                          look_ahead=look_ahead,\n",
    "                                          gamma=gamma,\n",
    "                                          mdp=mdp,\n",
    "                                          mt_mode=mt_mode,\n",
    "                                          prob_mode=prob_mode,\n",
    "                                          utility_mode=utility_mode,\n",
    "                                          debug=debug\n",
    "                                        )\n",
    "    \n",
    "    optimal_action = np.argmax(action_utilities)\n",
    "    \n",
    "    return optimal_action, action_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run train data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = np.arange(nS)\n",
    "mdp = MDP(mdp_transitions, mdp_rewards, initial_state=random.choice(initial_states), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_reward(mdp, state_values, start_state, rows, debug=False):\n",
    "    \"\"\"\n",
    "    Returns the average reward for the episode of length EPISODE_LENGTH \n",
    "    starting at `start_state`, when following the MARKOV TREE policy. \n",
    "    \n",
    "    Params:\n",
    "        - rows: data points for a single donor\n",
    "    \"\"\"    \n",
    "    mdp.reset(start_state)\n",
    "\n",
    "    s = start_state\n",
    "    rewards = []\n",
    "    for i, row in rows.iterrows():\n",
    "        if debug:\n",
    "            print('====Time step:', i, '| Current state:', s)\n",
    "            \n",
    "        # TODO: How to evaluate Markov Tree prospectively?\n",
    "        # FIXME: should be current state, not `start_row`\n",
    "        pre_action_state_probs, closest_state = get_mdp_state_probabilities(row)\n",
    "        post_action_state_utilities = [state_values[s] for s in range(nS)]\n",
    "                \n",
    "        optimal_action, action_utilities = get_optimal_markov_tree_action(mdp, pre_action_state_probs, post_action_state_utilities, \n",
    "                                       MT_MODE, PROB_MODE, UTILITY_MODE, \n",
    "                                       LOOK_AHEAD, GAMMA, debug=debug)\n",
    "        \n",
    "        if debug:\n",
    "            print('Optimal action', optimal_action, '   |    Final action utilities:', action_utilities)\n",
    "            \n",
    "        while optimal_action not in mdp.get_possible_actions(s):\n",
    "            action_utilities[optimal_action] = 0\n",
    "            optimal_action = np.argmax(action_utilities)\n",
    "        \n",
    "        if debug:\n",
    "            print('--Performing action:', optimal_action)\n",
    "    \n",
    "        s, r, done, _ = mdp.step(optimal_action)\n",
    "        rewards.append(r)\n",
    "    \n",
    "    if debug:\n",
    "        print('=============================================================================================')\n",
    "        \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Processed 0 rows ( 0.0 %) | Avg reward = nan ========== 0.0 mins\n",
      "====Time step: 1120225 | Current state: 0\n",
      "Optimal action 3    |    Final action utilities: [202.35650573853735, 243.81081270303488, 239.90337281465025, 244.96317643831594, 239.30010122707324, 240.71439475793576, 242.39650743165862]\n",
      "--Performing action: 3\n",
      "====Time step: 1120226 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.81827259174855, 255.88133773983478, 254.90041166754722, 254.98252007151126, 253.5304012511425, 253.1520493846676, 254.63813717496737]\n",
      "--Performing action: 1\n",
      "====Time step: 1120227 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.58719641673838, 244.82517079198067, 240.92304918653343, 245.98707451539383, 240.33300207353744, 241.71984634840936, 243.42011558539787]\n",
      "--Performing action: 3\n",
      "====Time step: 1120228 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.84424067464585, 252.14609169316773, 248.16309050785406, 253.25289956912513, 247.47740424046086, 248.95995391347893, 250.65613977789297]\n",
      "--Performing action: 3\n",
      "====Time step: 1120229 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.35859868346046, 249.8060389074583, 245.82492240792726, 250.9161297602182, 245.14313791561796, 246.64277978176904, 248.32004165656227]\n",
      "--Performing action: 3\n",
      "====Time step: 1120230 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.89976607422403, 247.5789146599814, 243.60044016891771, 248.69033640566477, 242.92076192426825, 244.43745413531155, 246.09591530010474]\n",
      "--Performing action: 3\n",
      "====Time step: 1120231 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.82731182809007, 247.20935529360273, 243.2323743714203, 248.32167768925896, 242.55427405399624, 244.07164362956004, 245.72790776180528]\n",
      "--Performing action: 3\n",
      "====Time step: 1120232 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.83894302254214, 247.2555140715512, 243.27902481235807, 248.36694319769333, 242.60016676032976, 244.11737623543198, 245.7737107840416]\n",
      "--Performing action: 3\n",
      "====Time step: 1120233 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.99093569544766, 248.0007882379614, 244.02304968801232, 249.11309395500197, 243.34478833449828, 244.85536469416766, 246.51869056488732]\n",
      "--Performing action: 3\n",
      "====Time step: 1120234 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.28548468873308, 249.48912118117664, 245.5061642061289, 250.59846914482353, 244.82264869135082, 246.3287046763648, 248.00167755238562]\n",
      "--Performing action: 3\n",
      "====Time step: 1120235 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.28008144612627, 249.4592940264495, 245.4765882627185, 250.5688639520793, 244.7934243926087, 246.299198676511, 247.97215109085826]\n",
      "--Performing action: 3\n",
      "====Time step: 1120236 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.18284974103176, 248.94501848509154, 244.96525452488453, 250.05309583756303, 244.28163016812894, 245.79013231068976, 247.45862092023046]\n",
      "--Performing action: 3\n",
      "====Time step: 1120237 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.14742018484029, 248.7852883358051, 244.8050513056914, 249.89422397214832, 244.1221258038755, 245.6319244700414, 247.2992640269381]\n",
      "--Performing action: 3\n",
      "====Time step: 1120238 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.02547766899596, 248.20020168775204, 244.22045965563254, 249.31337679087807, 243.5423110056637, 245.0526801694924, 246.71753652472415]\n",
      "--Performing action: 3\n",
      "====Time step: 1120239 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.1241024873126, 248.62600996940483, 244.64901985817906, 249.74368015554006, 243.97719997272867, 245.4748199447314, 247.1478542081774]\n",
      "--Performing action: 3\n",
      "====Time step: 1120240 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.83986139162465, 246.20029220149138, 242.28783966641873, 247.3573394924696, 241.68810498064045, 243.08047747528252, 244.7857755440833]\n",
      "--Performing action: 3\n",
      "====Time step: 1120241 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.93382079486818, 246.64284937506693, 242.73071471808322, 247.80107131396738, 242.13253635054627, 243.51883952831474, 245.22919709352712]\n",
      "--Performing action: 3\n",
      "====Time step: 1120242 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.9522049179948, 246.6507871701334, 242.74329429022677, 247.80907026996724, 242.14727375688835, 243.52717767034568, 245.239751154016]\n",
      "--Performing action: 3\n",
      "====Time step: 1120243 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.76325071238392, 245.7713346987927, 241.86263777133172, 246.92827664289547, 241.26436821728095, 242.6560379523632, 244.35894807716994]\n",
      "--Performing action: 3\n",
      "====Time step: 1120244 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.77284557391232, 245.8280014218586, 241.91864807780672, 246.98466260940972, 241.31978613565462, 242.71208113697998, 244.41504127264352]\n",
      "--Performing action: 3\n",
      "====Time step: 1120245 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.80615105117192, 245.99220048774055, 242.08252139621993, 247.1490526223256, 241.48376577939393, 242.87466977122713, 244.57914594328923]\n",
      "--Performing action: 3\n",
      "====Time step: 1120246 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.06282758304877, 247.20581984015013, 243.29667069637452, 248.36450684042475, 242.70049201001967, 244.07670146191225, 245.79399007243182]\n",
      "--Performing action: 3\n",
      "====Time step: 1120247 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.09806289640196, 247.38407570183364, 243.4743008981537, 248.54263605414346, 242.87774499565558, 244.25316952567914, 245.97176222263067]\n",
      "--Performing action: 3\n",
      "====Time step: 1120248 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.13634597707556, 247.57826777926547, 243.66777887024247, 248.73660586634122, 243.0707039368124, 244.44540770189747, 246.16535117820365]\n",
      "--Performing action: 3\n",
      "====Time step: 1120249 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.27558715770607, 258.3059491011123, 257.3100832499889, 257.39849855192836, 255.92438759580523, 255.5514441060596, 257.04783591848667]\n",
      "--Performing action: 1\n",
      "=============================================================================================\n",
      "Donor average reward. Markov Tree policy: 1.7862892255892238  |  Actual: 58.84 / 25 = 2.3536\n",
      "====Time step: 32875 | Current state: 2\n",
      "Optimal action 0    |    Final action utilities: [282.53059531604896, 279.5722979469927, 278.5091029290079, 278.57824094897603, 277.00241784760215, 276.6027524973456, 278.21123824645156]\n",
      "--Performing action: 0\n",
      "====Time step: 32876 | Current state: 2\n",
      "Optimal action 0    |    Final action utilities: [282.6282114788129, 280.1266929526919, 279.05823493185943, 279.13233465627616, 277.5489871728432, 277.15125121088465, 278.7623516215272]\n",
      "--Performing action: 0\n",
      "====Time step: 32877 | Current state: 2\n",
      "Optimal action 1    |    Final action utilities: [204.38600220782004, 261.99559540229933, 261.08883565596005, 261.00983625598997, 259.65731804198504, 259.2118776211418, 260.73192660345654]\n",
      "--Performing action: 0\n",
      "====Time step: 32878 | Current state: 2\n",
      "Optimal action 1    |    Final action utilities: [202.52336537078708, 253.2799066208762, 252.36765393407146, 252.35597867550752, 251.00004097338126, 250.58185658184726, 252.05822451018165]\n",
      "--Performing action: 0\n",
      "====Time step: 32879 | Current state: 2\n",
      "Optimal action 3    |    Final action utilities: [202.86862118117907, 246.74756961105828, 242.8088128475275, 247.85630159404766, 242.14381029945764, 243.61809624571364, 245.28498894350753]\n",
      "--Performing action: 0\n",
      "====Time step: 32880 | Current state: 2\n",
      "Optimal action 3    |    Final action utilities: [202.81351769881982, 246.43058615371558, 242.49503944351667, 247.5395686288201, 241.83167922644068, 243.30450735864184, 244.97004721839187]\n",
      "--Performing action: 0\n",
      "====Time step: 32881 | Current state: 2\n",
      "Optimal action 3    |    Final action utilities: [202.1432696645408, 242.75798013296082, 238.85061777055625, 243.86619906269405, 238.19816917534985, 239.67018170462921, 241.31356990185338]\n",
      "--Performing action: 0\n",
      "====Time step: 32882 | Current state: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action 3    |    Final action utilities: [202.25025511168516, 242.95289027585318, 239.06435400894995, 244.07144719485746, 238.4318857120512, 239.86549888987605, 241.52600331695098]\n",
      "--Performing action: 0\n",
      "====Time step: 32883 | Current state: 0\n",
      "Optimal action 1    |    Final action utilities: [202.1961061191742, 252.90845487382063, 251.7350584200727, 251.06273480603, 248.66370327161667, 249.91321211234253, 250.69792031669846]\n",
      "--Performing action: 1\n",
      "====Time step: 32884 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.16526444179377, 242.48464069067023, 238.59971042607282, 243.60072078449372, 237.96600060497096, 239.40204805541407, 241.05817641119995]\n",
      "--Performing action: 3\n",
      "====Time step: 32885 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.5394172309734, 253.82946733346745, 252.88823505312135, 252.8773542788662, 251.47647186596038, 251.1221668575931, 252.57211562570825]\n",
      "--Performing action: 1\n",
      "====Time step: 32886 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.7108496474733, 254.9494711532856, 253.9901521063519, 253.99058149966297, 252.56301949878156, 252.2292807167143, 253.67709969301075]\n",
      "--Performing action: 1\n",
      "====Time step: 32887 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.04573413717821, 256.8610556213478, 255.69256922623865, 255.00887154165252, 252.61724561634207, 253.82796405333, 254.6476998101926]\n",
      "--Performing action: 1\n",
      "====Time step: 32888 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.82504323810184, 255.5280695770851, 254.5665905732019, 254.5671284088504, 253.13635655282923, 252.80200558280768, 254.25292528231836]\n",
      "--Performing action: 1\n",
      "====Time step: 32889 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.5999705227371, 253.69358330282776, 252.77744562120182, 252.7489194513219, 251.38519654054338, 250.990420710084, 252.4553333528466]\n",
      "--Performing action: 1\n",
      "====Time step: 32890 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.79243447992906, 254.40223297410898, 253.4986992296938, 253.4683776684097, 252.1242819633954, 251.69390215960254, 253.17823572028195]\n",
      "--Performing action: 1\n",
      "====Time step: 32891 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.98517839962332, 255.3466206313281, 254.4415766538135, 254.41455018189004, 253.06869567805145, 252.6290886188167, 254.12275553790013]\n",
      "--Performing action: 1\n",
      "====Time step: 32892 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.9339825777872, 255.1242093900228, 254.21815120272692, 254.1966369013811, 252.84974566080166, 252.40887619480043, 253.90294407047733]\n",
      "--Performing action: 1\n",
      "====Time step: 32893 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.63443598347297, 253.53690276076543, 252.63998080047764, 252.6027089303356, 251.26794560512226, 250.8375829637376, 252.31658027904314]\n",
      "--Performing action: 1\n",
      "====Time step: 32894 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.67966472028812, 253.74706309217703, 252.8503362261026, 252.8108706068083, 251.4762253131807, 251.0456662536803, 252.5254128776168]\n",
      "--Performing action: 1\n",
      "====Time step: 32895 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [202.7673188223521, 254.16528952923414, 253.26844898876215, 253.2282069097418, 251.8934047240897, 251.45981591636775, 252.9428471932879]\n",
      "--Performing action: 1\n",
      "====Time step: 32896 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.23667023533045, 256.57873655839035, 255.67164938067975, 255.6473674224328, 254.298935586896, 253.84913565082036, 255.35388140022837]\n",
      "--Performing action: 1\n",
      "====Time step: 32897 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.3345614229277, 257.07016383000513, 256.16150884376714, 256.13738847836186, 254.7866536811763, 254.33561330812944, 255.84333345744037]\n",
      "--Performing action: 1\n",
      "====Time step: 32898 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.4407844554179, 257.61055266541337, 256.69977858194795, 256.6762168678664, 255.3223753157917, 254.87051848811421, 256.38132126364883]\n",
      "--Performing action: 1\n",
      "====Time step: 32899 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.7108643458743, 259.0622541005454, 258.14171481495924, 258.1271476796063, 256.7594280234487, 256.30719171206374, 257.8266713272002]\n",
      "--Performing action: 1\n",
      "=============================================================================================\n",
      "Donor average reward. Markov Tree policy: 2.7017213645367146  |  Actual: 3.8800000000000003 / 25 = 0.1552\n",
      "====Time step: 2241975 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.7897165836129, 246.70378089290892, 242.74593361872112, 247.81766997756557, 242.07805077497494, 243.5729575899556, 245.23417252704257]\n",
      "--Performing action: 3\n",
      "====Time step: 2241976 | Current state: 3\n",
      "Optimal action 1    |    Final action utilities: [203.10705481965164, 257.14977023331306, 256.1740253126851, 256.23108627756164, 254.78463540667087, 254.40808245174804, 255.8953977411017]\n",
      "--Performing action: 1\n",
      "====Time step: 2241977 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.81140632998486, 246.4846484505566, 242.546180225537, 247.6103554218144, 241.90006361720728, 243.35833416095028, 245.03400478040345]\n",
      "--Performing action: 3\n",
      "====Time step: 2241978 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.8206283161713, 246.56663795189831, 242.6259780470049, 247.69299030369098, 241.9796116890039, 243.4393312242773, 245.11520025012194]\n",
      "--Performing action: 3\n",
      "====Time step: 2241979 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.01485656885978, 247.93361131237384, 243.96624821135845, 249.0422099453341, 243.28852681442737, 244.78976733295326, 246.4547346814841]\n",
      "--Performing action: 3\n",
      "====Time step: 2241980 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [202.9074803030949, 247.31776161079173, 243.35675286239666, 248.43106291824847, 242.68694995729726, 244.1806722115726, 245.84582405332057]\n",
      "--Performing action: 3\n",
      "====Time step: 2241981 | Current state: 3\n",
      "Optimal action 3    |    Final action utilities: [203.0263311592205, 247.8489790967792, 243.88964237147434, 248.95564442526512, 243.2133666467698, 244.7067048000472, 246.37324756954405]\n",
      "--Performing action: 3\n",
      "====Time step: 2241982 | Current state: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-8c3e03622519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdonor_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdonor_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdonor_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_average_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'State_Cluster_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Donor average reward. Markov Tree policy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" |  Actual:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-b6892fcf31d5>\u001b[0m in \u001b[0;36mget_average_reward\u001b[1;34m(mdp, state_values, start_state, rows, debug)\u001b[0m\n\u001b[0;32m     22\u001b[0m         optimal_action, action_utilities = get_optimal_markov_tree_action(mdp, pre_action_state_probs, post_action_state_utilities, \n\u001b[0;32m     23\u001b[0m                                        \u001b[0mMT_MODE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPROB_MODE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUTILITY_MODE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                                        LOOK_AHEAD, GAMMA, debug=debug)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-9a1a237d840c>\u001b[0m in \u001b[0;36mget_optimal_markov_tree_action\u001b[1;34m(mdp, pre_probs, post_utilities, mt_mode, prob_mode, utility_mode, look_ahead, gamma, debug)\u001b[0m\n\u001b[0;32m     32\u001b[0m                                           \u001b[0mprob_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                                           \u001b[0mutility_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutility_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                                           \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                                         )\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-19970cdb6a23>\u001b[0m in \u001b[0;36mget_action_lookahead_utilities\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m                                                      \u001b[0mprob_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prob_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                                      \u001b[0mutility_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'utility_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                                      \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'debug'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                                                     )\n\u001b[0;32m     78\u001b[0m         \u001b[0maction_utilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mget_future_actions_utility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_action_utilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mt_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-19970cdb6a23>\u001b[0m in \u001b[0;36mget_action_lookahead_utilities\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m                                                      \u001b[0mprob_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prob_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                                      \u001b[0mutility_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'utility_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                                      \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'debug'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                                                     )\n\u001b[0;32m     78\u001b[0m         \u001b[0maction_utilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mget_future_actions_utility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_action_utilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mt_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-19970cdb6a23>\u001b[0m in \u001b[0;36mget_action_lookahead_utilities\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m                                                      \u001b[0mprob_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prob_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                                      \u001b[0mutility_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'utility_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                                      \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'debug'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m                                                     )\n\u001b[0;32m     78\u001b[0m         \u001b[0maction_utilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mget_future_actions_utility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_action_utilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mt_mode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-19970cdb6a23>\u001b[0m in \u001b[0;36mget_action_lookahead_utilities\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mpost_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mdp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next_state_transitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mpre_utilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_weighted_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'post_utilities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0maction_utilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_weighted_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_utilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pre_probs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-f3a8d2490c2a>\u001b[0m in \u001b[0;36mget_weighted_sum\u001b[1;34m(vals, weights)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_length_months = 25\n",
    "EPISODE_LENGTH = episode_length_months // TIME_PERIOD # total num months / TIME_PERIOD of each event\n",
    "\n",
    "MT_MODE = 'maximin' # 'maximax'  'maximin'\n",
    "PROB_MODE = 'hard' # 'hard' 'soft'\n",
    "UTILITY_MODE = 'dynamic' # 'static'  'dynamic'\n",
    "\n",
    "look_ahead_months = 8\n",
    "LOOK_AHEAD = look_ahead_months // TIME_PERIOD # num months lookahead / TIME_PERIOD\n",
    "\n",
    "# Discount factor γ / gamma\n",
    "# If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward. \n",
    "# If γ=1, the agent will evaluate each of its actions based on the sum total of all of its future rewards.\n",
    "GAMMA = 0.95\n",
    "\n",
    "debug = True\n",
    "\n",
    "# donor_sample_data = donor_data[donor_data['bloc'] == 0] # initial data rows\n",
    "\n",
    "donor_ids_sample = np.random.choice(donor_data['id'].unique(), 100)\n",
    "# donor_sample_data = donor_data[donor_data['id'].isin(donor_ids_sample)]\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "avg_rewards = []\n",
    "for i, donor_id in enumerate(donor_ids_sample):\n",
    "    if i % 100 == 0:\n",
    "        print('========== Processed', i, 'rows (', i / len(donor_initial_data) * 100,'%) |', \n",
    "              'Avg reward =',  np.mean(avg_rewards), '==========', (time.time() - start_time) / 60, 'mins')\n",
    "  \n",
    "    rows = donor_data[donor_data['id'] == donor_id]\n",
    "    \n",
    "    r = get_average_reward(mdp, state_values, rows.iloc[0]['State_Cluster_' + str(nS)], rows, debug)\n",
    "    print(\"Donor average reward. Markov Tree policy:\", r, \" |  Actual:\", sum(rows['reward']), '/', len(rows), '=', sum(rows['reward']) / len(rows))    \n",
    "\n",
    "    avg_rewards.append(r)    \n",
    "#     break\n",
    "\n",
    "print(\"Average reward for Markov Tree policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Average reward for Markov Tree with lookahead policy: \", np.mean(avg_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test data using Markov Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = './jmp/test/'\n",
    "test_rl_data_dir = './rl_data/test/'+ str(TIME_PERIOD) + '_month_period/'\n",
    "\n",
    "print('TIME_PERIOD:', TIME_PERIOD)\n",
    "print('nS:', nS)\n",
    "print('nA:', nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([state_values[s] for s in range(nS)])\n",
    "state_values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_donor_data = pd.read_csv(test_data_dir + 'all_data-' + str(TIME_PERIOD) + '_month_period.csv')\n",
    "\n",
    "# test_mdp_transitions[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_transitions = pickle.load(open(test_rl_data_dir + 'mdp_transitions_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_mdp_rewards[curr_state][action][next_state] - some have missing actions\n",
    "test_mdp_rewards = pickle.load(open(test_rl_data_dir + 'mdp_rewards_dict_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))\n",
    "# test_state_values[state]\n",
    "test_state_values = pickle.load(open(test_rl_data_dir + 'mdp_state_values_nS=' + str(nS) + '_nA=' + str(nA) + '.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
